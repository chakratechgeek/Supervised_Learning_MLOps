{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d6a11c",
   "metadata": {
    "papermill": {
     "duration": 0.01716,
     "end_time": "2024-07-22T14:59:15.067155",
     "exception": false,
     "start_time": "2024-07-22T14:59:15.049995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Supervised Learning\n",
    "\n",
    "**Types of Supervised Learning**\n",
    "  1. ***Regression***: is used when the target variable is continuous and we want to predict its value based on the input features. The goal is to learn a function that maps input features to a continuous output variable.\n",
    "  \n",
    "     * **Linear Regression**: Models the relationship between the dependent variable and one or more independent variables using a straight line.\n",
    "     * **Polynomial Regression**: Extends linear regression by modeling the relationship as an nth-degree polynomial.\n",
    "     * **Ridge Regression**: A type of linear regression that includes a regularization term to prevent overfitting.\n",
    "     * **Lasso Regression**: Similar to ridge regression but can also perform feature selection by setting some coefficients to zero.\n",
    "     * **Support Vector Regression (SVR)**: Uses support vector machine algorithms for regression problems.\n",
    "     * **Decision Trees and Random Forests**: Non-linear models that can capture complex relationships between features.\n",
    "\n",
    "\n",
    "  2. ***Classification***:  is used when the target variable is categorical and we want to predict which category or class the input features belong to. The goal is to learn a function that maps input features to discrete class labels.\n",
    "  \n",
    "     * **Logistic Regression**: Models the probability of a binary outcome using a logistic function.\n",
    "     * **K-Nearest Neighbors (KNN)**: Classifies data points based on the classes of their nearest neighbors.\n",
    "     * **Support Vector Machines (SVM)**: Finds the hyperplane that best separates the classes in the feature space.\n",
    "     * **Decision Trees**: Tree-based models that split the data into subsets based on feature values.\n",
    "     * **Random Forests**: An ensemble method that combines multiple decision trees to improve accuracy and prevent overfitting.\n",
    "     * **Naive Bayes**: A probabilistic classifier based on Bayes' theorem with strong independence assumptions between features.\n",
    "     * **Neural Networks**: Deep learning models that can capture complex patterns in data.\n",
    "     \n",
    "#####################################################################################################################################################\n",
    "1. **Data Collection and Preparation**\n",
    "\n",
    " * Gather data \n",
    " * Encode categorical variables if necessary.\n",
    " * Too many unique values in categorical feature is not used, reasons are below.\n",
    "    * **One-Hot Encoding** : Transforming a categorical feature with many unique values into one-hot encoded columns can significantly increase the dimensionality of the dataset.\n",
    "    * **Overfitting** : Models may overfit to the noise in high-cardinality features because they capture a lot of specific information that may not generalize well to new data. \n",
    "    * **Reduced Interpretability** : High-cardinality features can make models less interpretable because it becomes difficult to understand the influence of each unique value.\n",
    "    * **Computational Efficiency** : High-cardinality features increase the size of the dataset and the computational resources required for training, making it impractical for large datasets.\n",
    "\n",
    " * Missing Value handling\n",
    " \n",
    "2. **Exploratory Data Analysis (EDA)**\n",
    "\n",
    " * Understand data distribution (mean, median, mode, standard deviation).\n",
    "    * **Which Features** : Applicable for Numerical, Ordinal and Categorical features \n",
    "    * **Skewness** & **Kurtosis**\n",
    "    \n",
    " * Visualize data using histograms, box plots, scatter plots, etc.\n",
    " * Use various methods to remove outliers \n",
    " * Check for correlations & covariance between features.\n",
    "\n",
    "3. **Check for Assumptions of Linear Regression**\n",
    "\n",
    " * Linearity: The relationship between predictors and the target should be linear.\n",
    "    * **Check**: Scatter plots or correlation coefficients.\n",
    "    * **Measure** : Adjust the model or apply transformations if needed.\n",
    " * Independence: Observations should be independent of each other.\n",
    "    * **Check** : Study design and data collection process.\n",
    "    * **Measure** : Ensure proper randomization and data collection techniques.\n",
    " * Homoscedasticity: Constant variance of the residuals.\n",
    "    * **Check** : Residual plots.\n",
    "    * **Measure** : Plot residuals vs. fitted values and look for a random pattern.\n",
    " * Normality of Residuals: Residuals should be normally distributed.\n",
    "    * **Check** : Q-Q plots or normality tests (e.g., Shapiro-Wilk test).\n",
    "    * **Measure** : Transform data if residuals are not normally distributed.\n",
    " * No Multicollinearity: Predictors should not be highly correlated with each other.\n",
    "    * **Check** : Variance Inflation Factor (VIF).\n",
    "    * **Measure** : Remove or combine highly correlated predictors.\n",
    "\n",
    "\n",
    "4. **Feature Engineering**: Feature engineering is the process of creating new features or modifying existing ones to improve the performance of machine learning models. This involves domain knowledge and creativity to transform raw data into meaningful features.\n",
    "\n",
    "   * **Purpose**\n",
    "     * Enhance model performance by creating features that better represent the underlying patterns in the data.\n",
    "     * Provide additional information that the model can use to make more accurate predictions.\n",
    "     \n",
    "   * **Techniques**:\n",
    "     * **Transformation**: Applying mathematical operations (log, square root, etc.).\n",
    "     * **Encoding**: Converting categorical variables into numerical form (one-hot encoding, label encoding).\n",
    "     * **Binning**: Grouping continuous variables into bins or intervals.\n",
    "     * **Interaction Features**: Creating features that represent the interaction between other features.\n",
    "     * **Date/Time Features**: Extracting information from date and time variables (e.g., day of the week, month, hour).\n",
    "\n",
    "\n",
    "5. **Feature Selection**: Feature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. The goal is to improve model performance and reduce overfitting by removing irrelevant or redundant features.\n",
    "\n",
    "   * **Purpose**:\n",
    "     * Improve model performance by eliminating noise.\n",
    "     * Reduce overfitting by decreasing model complexity.\n",
    "     * Enhance model interpretability.\n",
    "     * Reduce computational cost.\n",
    "     \n",
    "   * **Techniques**:\n",
    "\n",
    "     1. **Filter Methods**: Select features based on statistical tests.\n",
    "        * Examples: Correlation, Chi-Square, ANOVA.   \n",
    "     2. **Wrapper Methods**: Select features based on model performance.\n",
    "        * Examples: Recursive Feature Elimination (RFE), Forward/Backward Selection.\n",
    "     3. **Embedded Methods**: Select features during model training.\n",
    "        * Examples: Lasso Regression, Tree-based methods (e.g., Random Forest, Gradient Boosting).\n",
    "\n",
    "\n",
    "6. **Feature Importance**:  Feature importance refers to techniques that assign a score to each feature based on how useful they are at predicting the target variable. This helps in understanding which features contribute the most to the predictions of the model.\n",
    "   \n",
    "   * **Purpose**:\n",
    "     * Identify the most influential features in making predictions.\n",
    "     * Understand the model's decision-making process.\n",
    "     * Perform feature selection based on importance scores.\n",
    "     \n",
    "   * **Techniques**:\n",
    "\n",
    "     * **Model-Based Methods**: Use models that inherently provide feature importance scores.\n",
    "       * Examples: Decision Trees, Random Forest, Gradient Boosting.\n",
    "       \n",
    "     * **Permutation Importance**: Measure the change in model performance when a feature's values are randomly shuffled.\n",
    "     * **Coefficients**: For linear models, the magnitude of the coefficients can indicate importance.\n",
    "\n",
    "\n",
    "7. **Model Training**\n",
    "\n",
    " * Split data into training and testing sets.\n",
    " * Train the multiple linear regression model.\n",
    "\n",
    "\n",
    "8. **Model Evaluation**\n",
    "\n",
    " * Evaluate the model using metrics like R-squared, adjusted R-squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\n",
    " * Check residual plots to ensure assumptions are met.\n",
    "\n",
    "\n",
    "9. **Model Interpretation**\n",
    "\n",
    " * Interpret the coefficients to understand the impact of each feature.\n",
    " * Check p-values to determine the significance of each feature.\n",
    "\n",
    "\n",
    "10. **Model Improvement**\n",
    "\n",
    " * Iterate by adding/removing features, transforming variables, or using different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845624b0",
   "metadata": {
    "papermill": {
     "duration": 0.014997,
     "end_time": "2024-07-22T14:59:15.097640",
     "exception": false,
     "start_time": "2024-07-22T14:59:15.082643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Encoding Categorical Variables\n",
    "In machine learning, many algorithms require input features to be numeric. Categorical variables, which are often strings or labels representing categories, need to be converted to a numeric format. This process is known as encoding. There are several methods to encode categorical variables, each suitable for different types of categorical data.\n",
    "\n",
    "**Common Methods for Encoding Categorical Variables**\n",
    "1. **Label Encoding**\n",
    "2. **One-Hot Encoding**\n",
    "3. **Ordinal Encoding**\n",
    "4. **Binary Encoding**\n",
    "5. **Frequency Encoding**\n",
    "6. **Target Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e7b40",
   "metadata": {
    "papermill": {
     "duration": 0.014971,
     "end_time": "2024-07-22T14:59:15.127951",
     "exception": false,
     "start_time": "2024-07-22T14:59:15.112980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Types in Statistics \n",
    "1. **Nominal Data**\n",
    "  **Definition**: is a type of categorical data where the categories do not have an intrinsic order or ranking. It is used to label variables without any quantitative value. Nominal data is purely qualitative and is used for naming or labeling different categories.\n",
    "  * **Characteristics**: No order, labels or names\n",
    "  * **Examples**       : Colors, Gender, Brands\n",
    "  \n",
    "  \n",
    "2. **Ordinal Data**\n",
    "  **Definition**: Ordinal data is categorical data where the categories have a meaningful order or ranking, but the intervals between the categories are not necessarily equal or known.\n",
    "  * **Characteristics**: Meaningful order, unequal intervals\n",
    "  * **Examples**       : Educational levels, Customer satisfaction\n",
    "  \n",
    "3. **Interval Data**\n",
    "  **Definition**: Interval data is numerical data where the intervals between values are equal, but there is no true zero point. This means that you can measure the difference between values, but not the ratio between them.\n",
    "  * **Characteristics**: Equal intervals, no true zero point\n",
    "  * **Examples**       : Temperature (Celsius/Fahrenheit), Dates\n",
    "  \n",
    "4. **Ratio Data**\n",
    "  **Definition**: Ratio data is numerical data where the intervals between values are equal, and there is a true zero point. This means that you can measure both differences and ratios between values.\n",
    "  * **Characteristics**: Equal intervals, true zero point\n",
    "  * **Examples**       : Height, Weight, Distance, Age\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e75ad96",
   "metadata": {
    "papermill": {
     "duration": 0.018635,
     "end_time": "2024-07-22T14:59:15.162336",
     "exception": false,
     "start_time": "2024-07-22T14:59:15.143701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Label Encoding \n",
    "Label Encoding is a technique used to convert categorical data into numerical data by assigning each unique category a different integer. It is a simple and effective method for encoding ordinal categorical variables, where the categories have a meaningful order. However, it can also be applied to nominal categorical variables, though with certain caveats.\n",
    "\n",
    "**How Label Encoding Works**\n",
    "Label Encoding assigns an integer value to each category in a column. For example, if you have a column with three categories: \"Red\", \"Green\", and \"Blue\", Label Encoding might assign 0 to \"Red\", 1 to \"Green\", and 2 to \"Blue\".\n",
    "\n",
    "**When to Use Label Encoding**\n",
    "\n",
    "Label Encoding is suitable for:\n",
    "\n",
    "1. **Ordinal Categorical Variables**:\n",
    "\n",
    "  * Variables that have a natural, meaningful order (e.g., \"Low\", \"Medium\", \"High\").\n",
    "  * Example: Rating scales, educational levels, size categories (small, medium, large).\n",
    "  \n",
    "2. **Features with a Small Number of Categories**:\n",
    "\n",
    "  * When the number of unique categories is relatively small, and the encoded integers won't introduce significant bias.\n",
    "  \n",
    "**When Not to Use Label Encoding**\n",
    "\n",
    "Label Encoding is not suitable for:\n",
    "\n",
    "1. **Nominal Categorical Variables with No Inherent Order**:\n",
    "\n",
    "  * Categories that do not have a meaningful order (e.g., \"Red\", \"Green\", \"Blue\").\n",
    "  * Label Encoding can introduce unintended ordinal relationships between categories (e.g., 0 < 1 < 2).\n",
    "  \n",
    "2. **Features with a Large Number of Categories**:\n",
    "\n",
    "  * When the number of unique categories is large, the encoded integers can create high cardinality and potentially introduce bias.\n",
    "\n",
    "3. **When the Model Assumes Ordinal Relationships**:\n",
    "\n",
    "  * Many machine learning models (e.g., linear regression, decision trees) can interpret the encoded integers as having an ordinal relationship, which may not be appropriate for nominal data\n",
    "  \n",
    "  \n",
    "When dealing with label encoded variables, you generally **should not** apply skewness, kurtosis, or transformations like log or Box-Cox. Here’s a detailed explanation of why these steps are usually unnecessary and can be misleading:\n",
    "\n",
    "***Example***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4691c818",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:15.195180Z",
     "iopub.status.busy": "2024-07-22T14:59:15.194772Z",
     "iopub.status.idle": "2024-07-22T14:59:17.292229Z",
     "shell.execute_reply": "2024-07-22T14:59:17.290880Z"
    },
    "papermill": {
     "duration": 2.116675,
     "end_time": "2024-07-22T14:59:17.294933",
     "exception": false,
     "start_time": "2024-07-22T14:59:15.178258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color  Color_Label\n",
      "0    Red            2\n",
      "1  Green            1\n",
      "2   Blue            0\n",
      "3  Green            1\n",
      "4    Red            2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'Color': ['Red', 'Green', 'Blue', 'Green', 'Red']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['Color_Label'] = label_encoder.fit_transform(df['Color'])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb83111",
   "metadata": {
    "papermill": {
     "duration": 0.015093,
     "end_time": "2024-07-22T14:59:17.325392",
     "exception": false,
     "start_time": "2024-07-22T14:59:17.310299",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. One-Hot Encoding\n",
    "One-Hot Encoding creates a separate binary column for each unique category in the original variable. For example, if you have a column with categories \"Red\", \"Green\", and \"Blue\", One-Hot Encoding will create three new columns, one for each color, with binary values indicating the presence or absence of each color.\n",
    "\n",
    "**When to Use One-Hot Encoding**\n",
    "One-Hot Encoding is suitable for:\n",
    "\n",
    "1. **Nominal Categorical Variables**:\n",
    "\n",
    "  * Variables where the categories do not have a meaningful order (e.g., colors, countries, product types).\n",
    "  \n",
    "2. **Categorical Variables with a Relatively Small Number of Categories**:\n",
    "\n",
    "  * When the number of unique categories is manageable, and creating multiple binary columns does not significantly increase the dimensionality of the dataset.\n",
    "  \n",
    "**When Not to Use One-Hot Encoding**\n",
    "One-Hot Encoding may not be suitable for:\n",
    "\n",
    "1. **Categorical Variables with a Large Number of Categories**:\n",
    "\n",
    "  * When the number of unique categories is very large (e.g., zip codes, user IDs), One-Hot Encoding can lead to a high-dimensional sparse matrix, which can be computationally expensive and may cause the model to overfit.\n",
    "  \n",
    "2. **Ordinal Categorical Variables**:\n",
    "\n",
    "  * For variables with a meaningful order (e.g., ratings, education levels), use Ordinal Encoding or Label Encoding instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3682245",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:17.358213Z",
     "iopub.status.busy": "2024-07-22T14:59:17.357614Z",
     "iopub.status.idle": "2024-07-22T14:59:17.378561Z",
     "shell.execute_reply": "2024-07-22T14:59:17.377226Z"
    },
    "papermill": {
     "duration": 0.040492,
     "end_time": "2024-07-22T14:59:17.380973",
     "exception": false,
     "start_time": "2024-07-22T14:59:17.340481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color_Blue  Color_Green  Color_Red\n",
      "0       False        False       True\n",
      "1       False         True      False\n",
      "2        True        False      False\n",
      "3       False         True      False\n",
      "4       False        False       True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#pip install --upgrade pandas\n",
    "\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'Color': ['Red', 'Green', 'Blue', 'Green', 'Red']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "df_encoded = pd.get_dummies(df, columns=['Color'])\n",
    "\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c034ec4b",
   "metadata": {
    "papermill": {
     "duration": 0.015036,
     "end_time": "2024-07-22T14:59:17.411303",
     "exception": false,
     "start_time": "2024-07-22T14:59:17.396267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Ordinal Encoding\n",
    "\n",
    "**Ordinal Encoding** is a technique used to convert categorical variables into numerical values, where the categories have a meaningful order or ranking. This method assigns a unique integer to each category based on its order. It is particularly useful for ordinal categorical variables, where the order of categories is important.\n",
    "\n",
    "**How Ordinal Encoding Works**\n",
    "  * ***Identify the Order***: Determine the order of the categories.\n",
    "  * ***Assign Integers***: Assign a unique integer to each category based on the order.\n",
    "  \n",
    "**When to Use Ordinal Encoding**\n",
    "1. **Ordinal Categorical Variables**:\n",
    "\n",
    "  * Variables where the categories have a meaningful order (e.g., \"Low\", \"Medium\", \"High\").\n",
    "  * Example: Ratings, education levels, size categories.\n",
    "  \n",
    "2. **Features with a Small Number of Categories**:\n",
    "\n",
    "  * When the number of unique categories is relatively small, making the encoding manageable.\n",
    "  \n",
    "**When Not to Use Ordinal Encoding**\n",
    "\n",
    "1. **Nominal Categorical Variables**:\n",
    "\n",
    "  * Variables where the categories do not have a meaningful order (e.g., \"Red\", \"Green\", \"Blue\").\n",
    "  * Use One-Hot Encoding instead for such variables.\n",
    "  \n",
    "2. **Features with a Large Number of Categories**:\n",
    "\n",
    "  * When the number of unique categories is large, consider using One-Hot Encoding, Frequency Encoding, or Target Encoding.\n",
    "  \n",
    "***Example***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baba26b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:17.445377Z",
     "iopub.status.busy": "2024-07-22T14:59:17.444398Z",
     "iopub.status.idle": "2024-07-22T14:59:17.456027Z",
     "shell.execute_reply": "2024-07-22T14:59:17.454351Z"
    },
    "papermill": {
     "duration": 0.031834,
     "end_time": "2024-07-22T14:59:17.458376",
     "exception": false,
     "start_time": "2024-07-22T14:59:17.426542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Size  Size_Ordinal\n",
      "0   Small             1\n",
      "1  Medium             2\n",
      "2   Large             3\n",
      "3  Medium             2\n",
      "4   Small             1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with an ordinal categorical variable\n",
    "data = {'Size': ['Small', 'Medium', 'Large', 'Medium', 'Small']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the order of categories\n",
    "size_mapping = {'Small': 1, 'Medium': 2, 'Large': 3}\n",
    "\n",
    "# Apply Ordinal Encoding\n",
    "df['Size_Ordinal'] = df['Size'].map(size_mapping)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b15a109",
   "metadata": {
    "papermill": {
     "duration": 0.015327,
     "end_time": "2024-07-22T14:59:17.489250",
     "exception": false,
     "start_time": "2024-07-22T14:59:17.473923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Binary Encoding\n",
    "\n",
    "Binary Encoding is a technique that combines the benefits of both Label Encoding and One-Hot Encoding. It first converts the category labels into integer values, then converts these integers into binary code, and finally splits the binary digits into separate columns. This method helps to reduce the dimensionality that can result from One-Hot Encoding, especially when dealing with ***categorical variables with many unique categories***.\n",
    "\n",
    "**How Binary Encoding Works**\n",
    "1. **Label Encoding**: Convert each category into a unique integer.\n",
    "2. **Binary Conversion**: Convert these integers into their binary representation.\n",
    "3. **Binary Columns**: Split the binary digits into separate columns.\n",
    "\n",
    "**When to Use Binary Encoding**\n",
    "1. Nominal Categorical Variables:\n",
    "\n",
    "  * Suitable for variables where the categories do not have a meaningful order (e.g., \"Red\", \"Green\", \"Blue\").\n",
    "  \n",
    "2. Features with a Large Number of Categories:\n",
    "\n",
    "  * Helps to reduce the dimensionality compared to One-Hot Encoding, making it practical for variables with many unique categories.\n",
    "  \n",
    "3. Mixed Data Types:\n",
    "\n",
    "  * Can be used when you have a mix of categorical and numerical data and need to maintain a manageable number of features.\n",
    "  \n",
    "**When Not to Use Binary Encoding**\n",
    "\n",
    "1. Ordinal Categorical Variables:\n",
    "  * For variables with a meaningful order, Ordinal Encoding might be more appropriate.\n",
    "  \n",
    "***Example***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3e53824",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:17.523164Z",
     "iopub.status.busy": "2024-07-22T14:59:17.522262Z",
     "iopub.status.idle": "2024-07-22T14:59:18.221992Z",
     "shell.execute_reply": "2024-07-22T14:59:18.220322Z"
    },
    "papermill": {
     "duration": 0.719515,
     "end_time": "2024-07-22T14:59:18.224591",
     "exception": false,
     "start_time": "2024-07-22T14:59:17.505076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Category_0  Category_1\n",
      "0           0           1\n",
      "1           1           0\n",
      "2           1           1\n",
      "3           0           1\n",
      "4           1           0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "\n",
    "# Sample DataFrame with a nominal categorical variable\n",
    "data = {'Category': ['A', 'B', 'C', 'A', 'B']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Binary Encoding\n",
    "binary_encoder = ce.BinaryEncoder(cols=['Category'])\n",
    "df_encoded = binary_encoder.fit_transform(df)\n",
    "\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311cb8d3",
   "metadata": {
    "papermill": {
     "duration": 0.015695,
     "end_time": "2024-07-22T14:59:18.255885",
     "exception": false,
     "start_time": "2024-07-22T14:59:18.240190",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Frequency Encoding\n",
    "Frequency Encoding is a technique used to convert categorical variables into numerical values by replacing each category with its frequency or count in the dataset. This method captures the distribution of categories and can be useful in certain machine learning models.\n",
    "\n",
    "**How Frequency Encoding Works**\n",
    "1. Calculate Frequencies: Count the occurrences of each category in the dataset.\n",
    "2. Replace Categories with Frequencies: Replace each category with its corresponding count.\n",
    "\n",
    "**When to Use Frequency Encoding**\n",
    "\n",
    "1. **High-Cardinality Categorical Variables**:\n",
    "\n",
    "  * When the categorical variable has many unique categories, Frequency Encoding can help reduce the dimensionality.\n",
    "  * Example: Zip codes, user IDs.\n",
    "  \n",
    "2. **Certain Machine Learning Models**:\n",
    "\n",
    "  * **Some models can benefit from frequency information, especially when the frequency of a category carries important information**.\n",
    "  * Example: Gradient Boosting Machines, Decision Trees.\n",
    "  \n",
    "**When Not to Use Frequency Encoding**\n",
    "\n",
    "1. **Ordinal Categorical Variables**:\n",
    "\n",
    "  * For ordinal data, where the order of categories matters, use Ordinal Encoding instead.\n",
    "  * Example: Ratings, education levels.\n",
    "  \n",
    "2. **Nominal Categorical Variables for Certain Models**:\n",
    "\n",
    "  * Some models may misinterpret the frequency as a ranking or ordinal value, which may not be appropriate.\n",
    "  * Example: Linear models where the magnitude of the encoded value could introduce unintended bias.\n",
    "  \n",
    "***Example***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db95df9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:18.289386Z",
     "iopub.status.busy": "2024-07-22T14:59:18.288348Z",
     "iopub.status.idle": "2024-07-22T14:59:18.304020Z",
     "shell.execute_reply": "2024-07-22T14:59:18.302576Z"
    },
    "papermill": {
     "duration": 0.035516,
     "end_time": "2024-07-22T14:59:18.306898",
     "exception": false,
     "start_time": "2024-07-22T14:59:18.271382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Encoding Mapping:\n",
      "Color\n",
      "Red      2\n",
      "Green    2\n",
      "Blue     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DataFrame with Frequency Encoding:\n",
      "   Color  Color_Frequency\n",
      "0    Red                2\n",
      "1  Green                2\n",
      "2   Blue                1\n",
      "3  Green                2\n",
      "4    Red                2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with a categorical variable\n",
    "data = {'Color': ['Red', 'Green', 'Blue', 'Green', 'Red']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the frequency of each category\n",
    "frequency_encoding = df['Color'].value_counts()\n",
    "print(\"Frequency Encoding Mapping:\")\n",
    "print(frequency_encoding)\n",
    "\n",
    "# Apply Frequency Encoding\n",
    "df['Color_Frequency'] = df['Color'].map(frequency_encoding)\n",
    "\n",
    "print(\"\\nDataFrame with Frequency Encoding:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bfc536",
   "metadata": {
    "papermill": {
     "duration": 0.015484,
     "end_time": "2024-07-22T14:59:18.338797",
     "exception": false,
     "start_time": "2024-07-22T14:59:18.323313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Target Encoding OR Mean Encoding\n",
    "\n",
    "**Target Encoding** (also known as Mean Encoding) is a technique used to convert categorical variables into numerical values by replacing each category with the mean of the target variable for that category. This method can capture the relationship between the categorical feature and the target variable, which can be particularly useful in supervised learning tasks.\n",
    "\n",
    "**How Target Encoding Works**\n",
    "1. **Calculate the Mean of the Target Variable for Each Category**: Group the data by the categorical variable and calculate the mean of the target variable for each group.\n",
    "2. **Replace Categories with the Calculated Mean**: Replace each category in the categorical variable with its corresponding mean of the target variable.\n",
    "\n",
    "**When to Use Target Encoding**\n",
    "1. **Supervised Learning Tasks**:\n",
    "\n",
    "  * When there is a strong relationship between the categorical feature and the target variable.\n",
    "  * Example: Predicting house prices, customer churn, etc.\n",
    "  \n",
    "2. **High-Cardinality Categorical Variables**:\n",
    "\n",
    "  * When the categorical variable has many unique categories, Target Encoding can help reduce dimensionality.\n",
    "  * Example: Product IDs, user IDs.\n",
    "  \n",
    "**When Not to Use Target Encoding**\n",
    "1. **Unsupervised Learning Tasks**:\n",
    "\n",
    "  * Target Encoding is not applicable as there is no target variable.\n",
    "  * Example: Clustering, dimensionality reduction.\n",
    "  \n",
    "2. **Data Leakage**:\n",
    "\n",
    "  * Be cautious of data leakage. The mean of the target variable should be calculated using only the training data and not the validation or test data.\n",
    "  * Example: If you use the entire dataset to calculate means, you might introduce information from the validation/test set into the training set.\n",
    "  \n",
    "***Example***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd244462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:18.372563Z",
     "iopub.status.busy": "2024-07-22T14:59:18.371460Z",
     "iopub.status.idle": "2024-07-22T14:59:18.505392Z",
     "shell.execute_reply": "2024-07-22T14:59:18.504350Z"
    },
    "papermill": {
     "duration": 0.154483,
     "end_time": "2024-07-22T14:59:18.508903",
     "exception": false,
     "start_time": "2024-07-22T14:59:18.354420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Target Encoding (Handling Data Leakage):\n",
      "   Color  Price  Color_Target\n",
      "0    Red    1.2           1.1\n",
      "1  Green    3.5           3.6\n",
      "2   Blue    2.8           NaN\n",
      "3  Green    3.6           3.5\n",
      "4    Red    1.1           1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18/83933954.py:25: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[3.6 1.2]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[val_index, 'Color_Target'] = val_data['Color'].map(target_mean)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Sample DataFrame with a categorical variable and a target variable\n",
    "data = {\n",
    "    'Color': ['Red', 'Green', 'Blue', 'Green', 'Red'],\n",
    "    'Price': [1.2, 3.5, 2.8, 3.6, 1.1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the KFold cross-validator\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a new column for the target encoded values\n",
    "df['Color_Target'] = 0\n",
    "\n",
    "for train_index, val_index in kf.split(df):\n",
    "    # Split the data into training and validation sets\n",
    "    train_data, val_data = df.iloc[train_index], df.iloc[val_index]\n",
    "    \n",
    "    # Calculate the mean of the target variable (Price) for each category in Color using the training set\n",
    "    target_mean = train_data.groupby('Color')['Price'].mean()\n",
    "    \n",
    "    # Apply the calculated means to the validation set\n",
    "    df.loc[val_index, 'Color_Target'] = val_data['Color'].map(target_mean)\n",
    "\n",
    "print(\"DataFrame with Target Encoding (Handling Data Leakage):\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14997e2",
   "metadata": {
    "papermill": {
     "duration": 0.015819,
     "end_time": "2024-07-22T14:59:18.540903",
     "exception": false,
     "start_time": "2024-07-22T14:59:18.525084",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Handling Missing Value\n",
    "\n",
    "**Types of Missing Values**\n",
    "\n",
    "1. **Missing Completely at Random (MCAR)**:\n",
    "\n",
    "   * Missing values have no relationship with any other data or variable in the dataset.\n",
    "   * Example: \n",
    "      * A survey respondent randomly skips a question due to oversight.\n",
    "      * During data entry, a few measurements are accidentally omitted or lost, such as recording temperatures or weights in a clinical trial, and these omissions are not related to the actual values.\n",
    "      * Random hardware malfunctions in a network of weather sensors cause occasional data points to be missed, regardless of the weather conditions.\n",
    "  \n",
    "  \n",
    "2. **Missing at Random (MAR)**:\n",
    "\n",
    "   * Missing values are related to some observed data but not to the missing data itself.\n",
    "   * Example: \n",
    "      *  Blood pressure readings are more likely to be missing for patients who do not visit the clinic regularly. However, the frequency of clinic visits is recorded and can explain the missing blood pressure data.\n",
    "      *   Salary information is missing more frequently for younger employees. However, the employees' ages and job titles are recorded, and these variables can explain the missing salary data.\n",
    "      * Test scores are missing more often for students with lower attendance rates. Attendance records are available and can help explain the pattern of missing test scores.\n",
    "     \n",
    "  \n",
    "3. **Missing Not at Random (MNAR)**:\n",
    "\n",
    "  * Missing values are related to the missing data itself.\n",
    "  * Example: \n",
    "      * Patients with severe depression may be less likely to report their mental health status, directly related to the severity of their depression.\n",
    "      * High-income individuals are less likely to disclose their exact earnings due to privacy concerns, so the missing income data is related to the income itself.\n",
    "      * Individuals who consume large amounts of alcohol might skip questions about their alcohol consumption, leading to missing data that is directly related to the amount of alcohol consumed.\n",
    "      \n",
    "      \n",
    "  \n",
    "# Methods for Halnding Missing Values\n",
    "\n",
    "Choosing the right method for handling missing values depends on the nature of the data, the amount and pattern of missingness, and the type of analysis or machine learning algorithm being used. Here’s a detailed guide on when to use each method:\n",
    "\n",
    "**Deletion Methods**: \n",
    "\n",
    "1. **Listwise Deletion**: Listwise deletion (also known as complete case analysis) removes entire rows from the dataset that contain any missing values. This method is straightforward but can result in a significant loss of data, especially if missing values are prevalent.\n",
    "\n",
    "   * **Use When**: The dataset has a small percentage of missing values and the missingness is completely at random (MCAR).\n",
    "   * **Advantages**: Simple and easy to implement. Maintains the integrity of the analysis.\n",
    "   * **Disadvantages**: Can lead to significant data loss, which may reduce statistical power and introduce bias if the missingness is not MCAR.\n",
    "  \n",
    "  \n",
    "2. **Pairwise Deletion**: Pairwise deletion (also known as available case analysis) retains all available data by only excluding missing values on a pairwise basis for specific analyses, such as correlation or covariance calculations. This method ensures that you use as much data as possible without completely discarding rows with missing values.\n",
    "\n",
    "   * **Use When**: Analyzing ***correlations or covariances*** and missing data is not excessive.\n",
    "   * **Advantages**: Retains more data compared to listwise deletion. Uses available data for each pairwise analysis.\n",
    "   * **Disadvantages**: Can lead to inconsistent sample sizes and biased results if the missingness is not MCAR.\n",
    "  \n",
    "  \n",
    "**Imputation Methods**\n",
    "\n",
    "\n",
    "1. **Mean/Median/Mode Imputation**: Use When\n",
    "\n",
    "   1.1 **Mean Imputation**:\n",
    "  \n",
    "     * Use when the data is ***symmetrically distributed***.\n",
    "     * Missing values are assumed to be missing completely at random (MCAR) or missing at random (MAR).\n",
    "     * The dataset is large enough that the mean is not significantly affected by the missing values.\n",
    "    \n",
    "   1.2 **Mediun Imputation**:\n",
    "  \n",
    "     * Use when the data is ***skewed*** (e.g., income, house prices).\n",
    "     * There are outliers present in the data.\n",
    "     * Missing values are assumed to be MCAR or MAR.\n",
    "    \n",
    "   1.3 **Mode Imputation**:\n",
    "  \n",
    "     * Use for ***categorical data***.\n",
    "     * The missing values are assumed to be MCAR or MAR.\n",
    "    \n",
    "    \n",
    "   * **Advantages**: Simple and fast to implement.\n",
    "   * **Disadvantages**: Reduces variability and can introduce bias if the missing values are not MCAR or MAR.\n",
    "  \n",
    "  \n",
    "2. **Forward/Backward Fill**: Forward fill and backward fill are simple imputation techniques used primarily in time series data. These methods fill missing values by carrying the last observed value forward or the next observed value backward.\n",
    "\n",
    "  * **Use When**: Working with ***time series*** data where it makes sense to carry forward or backward the last observed value.\n",
    "    * ***Forward Fill***  -  Suitable when you believe that the missing value should be the same as the last known value.\n",
    "    * ***Backward Fill*** -  Suitable when you believe that the missing value should be the same as the next known value.\n",
    "  * **Advantages**: Simple and preserves temporal sequence.\n",
    "  * **Disadvantages**: Can propagate errors if the missing data spans long periods.\n",
    "  \n",
    "  \n",
    "3. **K-Nearest Neighbors (KNN) Imputation**: is a method that replaces missing values by considering the 'k' nearest neighbors. This approach uses the ***similarities (distances) between data points*** to estimate missing values, making it capable of capturing more complex patterns in the data compared to simple imputation methods like mean or median.\n",
    "\n",
    "  * **Use When**: The dataset is relatively small to medium-sized, and the missingness is assumed to be MCAR or MAR.\n",
    "  * **Advantages**: Can capture more complex patterns in the data compared to simple imputation.\n",
    "  * **Disadvantages**: Computationally intensive and may not perform well with very large datasets or high-dimensional data.\n",
    "  \n",
    "  \n",
    "4. **Multivariate Imputation by Chained Equations (MICE)**:is a sophisticated technique that imputes missing values by modeling each feature with missing values as a function of other features in a dataset. The process is iterative and repeats the imputation process multiple times to improve accuracy.\n",
    "\n",
    "  * **Use When**: The dataset has a mix of numerical and categorical variables, and the missingness is assumed to be MAR.\n",
    "  * **Advantages**: Iteratively models each variable with missing data as a function of other variables, capturing complex relationships.\n",
    "  * **Disadvantages**: Computationally intensive and may require careful parameter tuning.\n",
    "  \n",
    "  \n",
    "5. **Regression Imputation**:  is a technique where missing values in a dataset are predicted using a regression model based on other variables. This method leverages the relationships between variables to estimate the missing data.\n",
    "\n",
    "  * **Use When**: There is a strong linear relationship between the variable with missing values and other variables. Typically used for numerical data but can be extended to categorical data with appropriate modeling.\n",
    "  * **Advantages**: Leverages the relationships between variables to estimate missing values.\n",
    "  * **Disadvantages**: Can underestimate variability and introduce bias if the relationships are not linear or if the missingness is not MAR.\n",
    "  \n",
    "  \n",
    "6. **Stochastic Regression Imputation**:\n",
    "\n",
    "  * **Use When**: \n",
    "    * **Complex Data Relationships**: When you need to capture complex relationships between variables and preserve the natural variability.\n",
    "    * **MAR (Missing At Random)**: Suitable when the missing data mechanism is MAR.\n",
    "    * **Preventing Overfitting**: When you want to avoid overfitting by adding variability to the imputed values.\n",
    "    \n",
    "  * **Advantages**: \n",
    "    * **Preserves Variability**: By adding a random error term, it maintains the natural variability in the data.\n",
    "    * **Reduces Bias**: The random error term helps to reduce the bias introduced by deterministic regression imputation.\n",
    "    * **Captures Relationships**: Utilizes the relationships between variables to provide accurate imputations.\n",
    "    \n",
    "    \n",
    "  * **Disadvantages**: More complex and computationally intensive.\n",
    " \n",
    " \n",
    "**Advanced Methods**\n",
    "1. **Interpolation**:\n",
    "\n",
    "  * **Use When**: The data is time series or spatial data where intermediate values can be estimated.\n",
    "  * **Advantages**: Can be more accurate than simple imputation methods for time series data.\n",
    "  * **Disadvantages**: May not be suitable for non-temporal or non-spatial data.\n",
    "  \n",
    "  \n",
    "2. **Maximum Likelihood**:\n",
    "\n",
    "  * **Use When**: The data follows a known distribution and the parameters of the distribution can be estimated.\n",
    "  * **Advantages**: Statistically efficient and can handle MAR and MNAR.\n",
    "  * **Disadvantages**: Requires a good understanding of the data distribution and more complex to implement.\n",
    "  \n",
    "  \n",
    "3. **Machine Learning Algorithms**:\n",
    "\n",
    "  * **Use When**: The dataset is large and complex, and simple imputation methods are insufficient.\n",
    "  * **Advantages**: Can capture complex patterns and interactions in the data.\n",
    "  * **Disadvantages**: Computationally intensive and requires careful tuning and validation.\n",
    "\n",
    "\n",
    "\n",
    "**Flag and Fill**\n",
    "\n",
    "1. **Flag Missing Values**:\n",
    "\n",
    "  * **Use When**: You want to retain information about which values were missing.\n",
    "  * **Advantages**: Adds a binary indicator for missing values, which can be useful for certain models.\n",
    "  * **Disadvantages**: Adds additional columns, increasing dimensionality.\n",
    "  \n",
    "  \n",
    "2. **Fill Missing Values**:\n",
    "\n",
    "  * **Use When**: After flagging missing values, you impute them using one of the methods above.\n",
    "  * **Advantages**: Combines the benefits of imputation with the information about missingness.\n",
    "  * **Disadvantages**: Requires careful handling to avoid overfitting or bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc49d55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:18.574711Z",
     "iopub.status.busy": "2024-07-22T14:59:18.574028Z",
     "iopub.status.idle": "2024-07-22T14:59:18.588090Z",
     "shell.execute_reply": "2024-07-22T14:59:18.586923Z"
    },
    "papermill": {
     "duration": 0.033842,
     "end_time": "2024-07-22T14:59:18.590553",
     "exception": false,
     "start_time": "2024-07-22T14:59:18.556711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "     A    B    C\n",
      "0  1.0  NaN    a\n",
      "1  2.0  2.0    b\n",
      "2  NaN  3.0    b\n",
      "3  4.0  4.0  NaN\n",
      "4  5.0  NaN    a\n",
      "\n",
      "DataFrame After Listwise Deletion:\n",
      "     A    B  C\n",
      "1  2.0  2.0  b\n"
     ]
    }
   ],
   "source": [
    "#Deletion Method \n",
    "#Likewise Deletion Method \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample DataFrame with missing values\n",
    "data = {\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [np.nan, 2, 3, 4, np.nan],\n",
    "    'C': ['a', 'b', 'b', np.nan, 'a']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "# Apply Listwise Deletion\n",
    "df_listwise = df.dropna()\n",
    "\n",
    "print(\"\\nDataFrame After Listwise Deletion:\")\n",
    "print(df_listwise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a41cb2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:18.624702Z",
     "iopub.status.busy": "2024-07-22T14:59:18.623713Z",
     "iopub.status.idle": "2024-07-22T14:59:18.884840Z",
     "shell.execute_reply": "2024-07-22T14:59:18.883458Z"
    },
    "papermill": {
     "duration": 0.282576,
     "end_time": "2024-07-22T14:59:18.889026",
     "exception": false,
     "start_time": "2024-07-22T14:59:18.606450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "    X1   X2    X3     X4\n",
      "0  1.2  NaN   1.0   10.0\n",
      "1  2.3  3.3   2.0   20.0\n",
      "2  NaN  4.4   3.0   30.0\n",
      "3  4.5  NaN   4.0   40.0\n",
      "4  5.6  6.6   5.0    NaN\n",
      "5  6.7  7.7  66.0  600.0\n",
      "6  7.8  8.8   NaN   70.0\n",
      "\n",
      "Pairwise Correlation Matrix:\n",
      "          X1        X2        X3        X4\n",
      "X1  1.000000  1.000000  0.688704  0.517901\n",
      "X2  1.000000  1.000000  0.756889  0.488646\n",
      "X3  0.688704  0.756889  1.000000  0.999992\n",
      "X4  0.517901  0.488646  0.999992  1.000000\n",
      "\n",
      "DataFrame After Filling Missing Values in X1:\n",
      "         X1   X2    X3     X4\n",
      "0  1.200000  NaN   1.0   10.0\n",
      "1  2.300000  3.3   2.0   20.0\n",
      "2  2.483333  4.4   3.0   30.0\n",
      "3  4.500000  NaN   4.0   40.0\n",
      "4  5.600000  6.6   5.0    NaN\n",
      "5  6.700000  7.7  66.0  600.0\n",
      "6  7.800000  8.8   NaN   70.0\n",
      "\n",
      "DataFrame After Filling Missing Values in X2:\n",
      "         X1        X2    X3     X4\n",
      "0  1.200000  2.732644   1.0   10.0\n",
      "1  2.300000  3.300000   2.0   20.0\n",
      "2  2.483333  4.400000   3.0   30.0\n",
      "3  4.500000  5.727421   4.0   40.0\n",
      "4  5.600000  6.600000   5.0    NaN\n",
      "5  6.700000  7.700000  66.0  600.0\n",
      "6  7.800000  8.800000   NaN   70.0\n",
      "\n",
      "DataFrame After Filling Missing Values in X3:\n",
      "         X1        X2    X3     X4\n",
      "0  1.200000  2.732644   1.0   10.0\n",
      "1  2.300000  3.300000   2.0   20.0\n",
      "2  2.483333  4.400000   3.0   30.0\n",
      "3  4.500000  5.727421   4.0   40.0\n",
      "4  5.600000  6.600000   5.0    NaN\n",
      "5  6.700000  7.700000  66.0  600.0\n",
      "6  7.800000  8.800000  13.5   70.0\n",
      "\n",
      "DataFrame After Filling Missing Values in X4:\n",
      "         X1        X2    X3          X4\n",
      "0  1.200000  2.732644   1.0   10.000000\n",
      "1  2.300000  3.300000   2.0   20.000000\n",
      "2  2.483333  4.400000   3.0   30.000000\n",
      "3  4.500000  5.727421   4.0   40.000000\n",
      "4  5.600000  6.600000   5.0  197.248031\n",
      "5  6.700000  7.700000  66.0  600.000000\n",
      "6  7.800000  8.800000  13.5   70.000000\n",
      "\n",
      "Final DataFrame After Filling All Missing Values:\n",
      "         X1        X2    X3          X4\n",
      "0  1.200000  2.732644   1.0   10.000000\n",
      "1  2.300000  3.300000   2.0   20.000000\n",
      "2  2.483333  4.400000   3.0   30.000000\n",
      "3  4.500000  5.727421   4.0   40.000000\n",
      "4  5.600000  6.600000   5.0  197.248031\n",
      "5  6.700000  7.700000  66.0  600.000000\n",
      "6  7.800000  8.800000  13.5   70.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18/3031174830.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  not_missing_X1['X2'] = imputer.fit_transform(not_missing_X1[['X2']])\n",
      "/tmp/ipykernel_18/3031174830.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_X1['X2'] = imputer.transform(missing_X1[['X2']])\n",
      "/tmp/ipykernel_18/3031174830.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  not_missing_X2['X1'] = imputer.fit_transform(not_missing_X2[['X1']])\n",
      "/tmp/ipykernel_18/3031174830.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_X2['X1'] = imputer.transform(missing_X2[['X1']])\n",
      "/tmp/ipykernel_18/3031174830.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  not_missing_X4['X1'] = imputer.fit_transform(not_missing_X4[['X1']])\n",
      "/tmp/ipykernel_18/3031174830.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_X4['X1'] = imputer.transform(missing_X4[['X1']])\n"
     ]
    }
   ],
   "source": [
    "#Deletion Method \n",
    "#Pairwise Deletion Method \n",
    "# Create a Sample DataFrame with Missing Values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample DataFrame with missing values\n",
    "data = {\n",
    "    'X1': [1.2, 2.3, np.nan, 4.5, 5.6, 6.7, 7.8],\n",
    "    'X2': [np.nan, 3.3, 4.4, np.nan, 6.6, 7.7, 8.8],  # Well-correlated with X1\n",
    "    'X3': [1, 2, 3, 4, 5, 66, np.nan],  # Poorly correlated with X1 and X2\n",
    "    'X4': [10, 20, 30, 40, np.nan, 600, 70]  # Moderately correlated with X1 and X2\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Calculate the pairwise correlation matrix\n",
    "correlation_matrix = df.corr(method='pearson', min_periods=1)\n",
    "\n",
    "print(\"\\nPairwise Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "#Filling Missing Values in Column X1 Using Column X2\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Impute missing values for predictor columns\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Data where X1 is not missing\n",
    "not_missing_X1 = df[df['X1'].notna()]\n",
    "\n",
    "# Data where X1 is missing\n",
    "missing_X1 = df[df['X1'].isna()]\n",
    "\n",
    "# Impute missing values in X2 using the mean\n",
    "not_missing_X1['X2'] = imputer.fit_transform(not_missing_X1[['X2']])\n",
    "\n",
    "# Fit a linear regression model to predict X1 using X2\n",
    "model_X1 = LinearRegression()\n",
    "model_X1.fit(not_missing_X1[['X2']], not_missing_X1['X1'])\n",
    "\n",
    "# Impute missing values in X2 for the rows where X1 is missing\n",
    "missing_X1['X2'] = imputer.transform(missing_X1[['X2']])\n",
    "\n",
    "# Predict missing values of X1\n",
    "predicted_X1 = model_X1.predict(missing_X1[['X2']])\n",
    "\n",
    "# Fill the missing values in X1\n",
    "df.loc[df['X1'].isna(), 'X1'] = predicted_X1\n",
    "\n",
    "print(\"\\nDataFrame After Filling Missing Values in X1:\")\n",
    "print(df)\n",
    "\n",
    "#Filling Missing Values in Column X2 Using Column X1\n",
    "# Data where X2 is not missing\n",
    "not_missing_X2 = df[df['X2'].notna()]\n",
    "\n",
    "# Data where X2 is missing\n",
    "missing_X2 = df[df['X2'].isna()]\n",
    "\n",
    "# Impute missing values in X1 using the mean\n",
    "not_missing_X2['X1'] = imputer.fit_transform(not_missing_X2[['X1']])\n",
    "\n",
    "# Fit a linear regression model to predict X2 using X1\n",
    "model_X2 = LinearRegression()\n",
    "model_X2.fit(not_missing_X2[['X1']], not_missing_X2['X2'])\n",
    "\n",
    "# Impute missing values in X1 for the rows where X2 is missing\n",
    "missing_X2['X1'] = imputer.transform(missing_X2[['X1']])\n",
    "\n",
    "# Predict missing values of X2\n",
    "predicted_X2 = model_X2.predict(missing_X2[['X1']])\n",
    "\n",
    "# Fill the missing values in X2\n",
    "df.loc[df['X2'].isna(), 'X2'] = predicted_X2\n",
    "\n",
    "print(\"\\nDataFrame After Filling Missing Values in X2:\")\n",
    "print(df)\n",
    "\n",
    "#Filling Missing Values in Column X3 Using Mean Imputation\n",
    "# Impute missing values in X3 using the mean\n",
    "df['X3'] = imputer.fit_transform(df[['X3']])\n",
    "\n",
    "print(\"\\nDataFrame After Filling Missing Values in X3:\")\n",
    "print(df)\n",
    "\n",
    "#Filling Missing Values in Column X4 Using Column X1\n",
    "# Data where X4 is not missing\n",
    "not_missing_X4 = df[df['X4'].notna()]\n",
    "\n",
    "# Data where X4 is missing\n",
    "missing_X4 = df[df['X4'].isna()]\n",
    "\n",
    "# Impute missing values in X1 using the mean\n",
    "not_missing_X4['X1'] = imputer.fit_transform(not_missing_X4[['X1']])\n",
    "\n",
    "# Fit a linear regression model to predict X4 using X1\n",
    "model_X4 = LinearRegression()\n",
    "model_X4.fit(not_missing_X4[['X1']], not_missing_X4['X4'])\n",
    "\n",
    "# Impute missing values in X1 for the rows where X4 is missing\n",
    "missing_X4['X1'] = imputer.transform(missing_X4[['X1']])\n",
    "\n",
    "# Predict missing values of X4\n",
    "predicted_X4 = model_X4.predict(missing_X4[['X1']])\n",
    "\n",
    "# Fill the missing values in X4\n",
    "df.loc[df['X4'].isna(), 'X4'] = predicted_X4\n",
    "\n",
    "print(\"\\nDataFrame After Filling Missing Values in X4:\")\n",
    "print(df)\n",
    "\n",
    "#Final DF\n",
    "print(\"\\nFinal DataFrame After Filling All Missing Values:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc7ab7d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:18.924009Z",
     "iopub.status.busy": "2024-07-22T14:59:18.923253Z",
     "iopub.status.idle": "2024-07-22T14:59:18.967303Z",
     "shell.execute_reply": "2024-07-22T14:59:18.965883Z"
    },
    "papermill": {
     "duration": 0.064668,
     "end_time": "2024-07-22T14:59:18.970027",
     "exception": false,
     "start_time": "2024-07-22T14:59:18.905359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Symmetrical DataFrame:\n",
      "    Age    Income\n",
      "0  25.0   50000.0\n",
      "1  30.0   60000.0\n",
      "2   NaN   70000.0\n",
      "3  35.0       NaN\n",
      "4  40.0   90000.0\n",
      "5  45.0  100000.0\n",
      "6   NaN  110000.0\n",
      "\n",
      "DataFrame After Mean Imputation:\n",
      "    Age    Income\n",
      "0  25.0   50000.0\n",
      "1  30.0   60000.0\n",
      "2  35.0   70000.0\n",
      "3  35.0   80000.0\n",
      "4  40.0   90000.0\n",
      "5  45.0  100000.0\n",
      "6  35.0  110000.0\n",
      "\n",
      "Original Skewed DataFrame:\n",
      "    Age  Income\n",
      "0  25.0   50000\n",
      "1  30.0   60000\n",
      "2   NaN   70000\n",
      "3  35.0  800000\n",
      "4  40.0   90000\n",
      "5  45.0  100000\n",
      "6   NaN  110000\n",
      "\n",
      "DataFrame After Median Imputation:\n",
      "    Age    Income\n",
      "0  25.0   50000.0\n",
      "1  30.0   60000.0\n",
      "2  35.0   70000.0\n",
      "3  35.0  800000.0\n",
      "4  40.0   90000.0\n",
      "5  45.0  100000.0\n",
      "6  35.0  110000.0\n",
      "\n",
      "Original Categorical DataFrame:\n",
      "  Category\n",
      "0        A\n",
      "1        B\n",
      "2        B\n",
      "3        A\n",
      "4      NaN\n",
      "5        B\n",
      "6        A\n",
      "\n",
      "DataFrame After Mode Imputation:\n",
      "  Category\n",
      "0        A\n",
      "1        B\n",
      "2        B\n",
      "3        A\n",
      "4      NaN\n",
      "5        B\n",
      "6        A\n"
     ]
    }
   ],
   "source": [
    "#MEAN/MEDIAN/MODE Imputation example\n",
    "#Scenario 1: Symmetrically Distributed Data (Mean Imputation)\n",
    "#DataFrame with Symmetrically Distributed Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample DataFrame with missing values\n",
    "data_sym = {\n",
    "    'Age': [25, 30, np.nan, 35, 40, 45, np.nan],\n",
    "    'Income': [50000, 60000, 70000, np.nan, 90000, 100000, 110000]\n",
    "}\n",
    "df_sym = pd.DataFrame(data_sym)\n",
    "\n",
    "print(\"Original Symmetrical DataFrame:\")\n",
    "print(df_sym)\n",
    "\n",
    "#Apply Mean Imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Mean Imputation for Age and Income columns\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "df_sym['Age'] = mean_imputer.fit_transform(df_sym[['Age']])\n",
    "df_sym['Income'] = mean_imputer.fit_transform(df_sym[['Income']])\n",
    "\n",
    "print(\"\\nDataFrame After Mean Imputation:\")\n",
    "print(df_sym)\n",
    "\n",
    "#Scenario 2: Skewed Data with Outliers (Median Imputation)\n",
    "#DataFrame with Skewed Data and Outliers\n",
    "# Create a sample DataFrame with missing values and skewed data\n",
    "data_skew = {\n",
    "    'Age': [25, 30, np.nan, 35, 40, 45, np.nan],\n",
    "    'Income': [50000, 60000, 70000, 800000, 90000, 100000, 110000]  # Outlier in Income\n",
    "}\n",
    "df_skew = pd.DataFrame(data_skew)\n",
    "\n",
    "print(\"\\nOriginal Skewed DataFrame:\")\n",
    "print(df_skew)\n",
    "#Apply Median Imputation\n",
    "# Median Imputation for Age and Income columns\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "df_skew['Age'] = median_imputer.fit_transform(df_skew[['Age']])\n",
    "df_skew['Income'] = median_imputer.fit_transform(df_skew[['Income']])\n",
    "\n",
    "print(\"\\nDataFrame After Median Imputation:\")\n",
    "print(df_skew)\n",
    "\n",
    "#Scenario 3: Categorical Data (Mode Imputation)\n",
    "#DataFrame with Categorical Data\n",
    "# Create a sample DataFrame with missing values in a categorical column\n",
    "data_cat = {\n",
    "    'Category': ['A', 'B', 'B', 'A', np.nan, 'B', 'A']\n",
    "}\n",
    "df_cat = pd.DataFrame(data_cat)\n",
    "\n",
    "print(\"\\nOriginal Categorical DataFrame:\")\n",
    "print(df_cat)\n",
    "#Apply Mode Imputation\n",
    "# Mode Imputation for Category column\n",
    "#mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "#df_cat['Category'] = mode_imputer.fit_transform(df_cat[['Category']])\n",
    "\n",
    "print(\"\\nDataFrame After Mode Imputation:\")\n",
    "print(df_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3efcae17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:19.005874Z",
     "iopub.status.busy": "2024-07-22T14:59:19.004899Z",
     "iopub.status.idle": "2024-07-22T14:59:19.032801Z",
     "shell.execute_reply": "2024-07-22T14:59:19.031752Z"
    },
    "papermill": {
     "duration": 0.050653,
     "end_time": "2024-07-22T14:59:19.037332",
     "exception": false,
     "start_time": "2024-07-22T14:59:18.986679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Time Series DataFrame:\n",
      "        Date  Stock_Price\n",
      "0 2023-01-01        100.0\n",
      "1 2023-01-02          NaN\n",
      "2 2023-01-03          NaN\n",
      "3 2023-01-04        105.0\n",
      "4 2023-01-05          NaN\n",
      "5 2023-01-06        110.0\n",
      "6 2023-01-07          NaN\n",
      "7 2023-01-08          NaN\n",
      "8 2023-01-09          NaN\n",
      "9 2023-01-10        115.0\n",
      "\n",
      "DataFrame After Forward Fill Imputation:\n",
      "        Date  Stock_Price\n",
      "0 2023-01-01        100.0\n",
      "1 2023-01-02        100.0\n",
      "2 2023-01-03        100.0\n",
      "3 2023-01-04        105.0\n",
      "4 2023-01-05        105.0\n",
      "5 2023-01-06        110.0\n",
      "6 2023-01-07        110.0\n",
      "7 2023-01-08        110.0\n",
      "8 2023-01-09        110.0\n",
      "9 2023-01-10        115.0\n",
      "\n",
      "DataFrame After Backward Fill Imputation:\n",
      "        Date  Stock_Price\n",
      "0 2023-01-01        100.0\n",
      "1 2023-01-02        105.0\n",
      "2 2023-01-03        105.0\n",
      "3 2023-01-04        105.0\n",
      "4 2023-01-05        110.0\n",
      "5 2023-01-06        110.0\n",
      "6 2023-01-07        115.0\n",
      "7 2023-01-08        115.0\n",
      "8 2023-01-09        115.0\n",
      "9 2023-01-10        115.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18/1932433021.py:18: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_ffill['Stock_Price'] = df_ffill['Stock_Price'].fillna(method='ffill')\n",
      "/tmp/ipykernel_18/1932433021.py:26: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_bfill['Stock_Price'] = df_bfill['Stock_Price'].fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "#Forward/Backward Fill\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample time series DataFrame with missing values\n",
    "date_range = pd.date_range(start='2023-01-01', periods=10, freq='D')\n",
    "data = {\n",
    "    'Date': date_range,\n",
    "    'Stock_Price': [100, np.nan, np.nan, 105, np.nan, 110, np.nan, np.nan, np.nan, 115]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original Time Series DataFrame:\")\n",
    "print(df)\n",
    "# Apply Forward Fill Imputation\n",
    "# Forward Fill Imputation\n",
    "df_ffill = df.copy()\n",
    "df_ffill['Stock_Price'] = df_ffill['Stock_Price'].fillna(method='ffill')\n",
    "\n",
    "print(\"\\nDataFrame After Forward Fill Imputation:\")\n",
    "print(df_ffill)\n",
    "\n",
    "# Apply Backward Fill Imputation\n",
    "# Backward Fill Imputation\n",
    "df_bfill = df.copy()\n",
    "df_bfill['Stock_Price'] = df_bfill['Stock_Price'].fillna(method='bfill')\n",
    "\n",
    "print(\"\\nDataFrame After Backward Fill Imputation:\")\n",
    "print(df_bfill)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0a18834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:19.073603Z",
     "iopub.status.busy": "2024-07-22T14:59:19.072629Z",
     "iopub.status.idle": "2024-07-22T14:59:19.107200Z",
     "shell.execute_reply": "2024-07-22T14:59:19.105758Z"
    },
    "papermill": {
     "duration": 0.055514,
     "end_time": "2024-07-22T14:59:19.109791",
     "exception": false,
     "start_time": "2024-07-22T14:59:19.054277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame with Missing Values:\n",
      "   Rooms  LotSize  YearBuilt     Price\n",
      "0    3.0   5000.0     2000.0  300000.0\n",
      "1    4.0   6000.0     1995.0  400000.0\n",
      "2    NaN   7000.0     2010.0  500000.0\n",
      "3    3.0      NaN     2005.0  350000.0\n",
      "4    5.0  10000.0     2015.0       NaN\n",
      "5    4.0   8000.0     2000.0  450000.0\n",
      "6    NaN   6000.0     2005.0  420000.0\n",
      "7    3.0      NaN     2010.0  390000.0\n",
      "8    4.0   7000.0        NaN  410000.0\n",
      "9    5.0   9000.0     2015.0       NaN\n",
      "\n",
      "DataFrame After KNN Imputation:\n",
      "   Rooms  LotSize  YearBuilt     Price\n",
      "0    3.0   5000.0     2000.0  300000.0\n",
      "1    4.0   6000.0     1995.0  400000.0\n",
      "2    5.0   7000.0     2010.0  500000.0\n",
      "3    3.0   9500.0     2005.0  350000.0\n",
      "4    5.0  10000.0     2015.0  370000.0\n",
      "5    4.0   8000.0     2000.0  450000.0\n",
      "6    5.0   6000.0     2005.0  420000.0\n",
      "7    3.0   9500.0     2010.0  390000.0\n",
      "8    4.0   7000.0     2015.0  410000.0\n",
      "9    5.0   9000.0     2015.0  370000.0\n"
     ]
    }
   ],
   "source": [
    "#K-Nearest Neighbors (KNN) Imputation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a realistic sample DataFrame with missing values\n",
    "data = {\n",
    "    'Rooms': [3, 4, np.nan, 3, 5, 4, np.nan, 3, 4, 5],\n",
    "    'LotSize': [5000, 6000, 7000, np.nan, 10000, 8000, 6000, np.nan, 7000, 9000],\n",
    "    'YearBuilt': [2000, 1995, 2010, 2005, 2015, 2000, 2005, 2010, np.nan, 2015],\n",
    "    'Price': [300000, 400000, 500000, 350000, np.nan, 450000, 420000, 390000, 410000, np.nan]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame with Missing Values:\")\n",
    "print(df)\n",
    "#To impute the missing values using KNN imputation, we need to initialize the KNNImputer and apply it to our DataFrame.\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Initialize the KNN imputer with a chosen number of neighbors\n",
    "knn_imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "# Apply the KNN imputer to the DataFrame\n",
    "df_imputed = pd.DataFrame(knn_imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"\\nDataFrame After KNN Imputation:\")\n",
    "print(df_imputed)\n",
    "\n",
    "#Rooms: Missing values are imputed based on the average of the nearest neighbors' Rooms.\n",
    "#LotSize: Missing values are imputed based on the average of the nearest neighbors' LotSize.\n",
    "#YearBuilt: Missing values are imputed based on the average of the nearest neighbors' YearBuilt.\n",
    "#Price: Missing values are imputed based on the average of the nearest neighbors' Price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3337b2b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:19.146311Z",
     "iopub.status.busy": "2024-07-22T14:59:19.145900Z",
     "iopub.status.idle": "2024-07-22T14:59:19.204594Z",
     "shell.execute_reply": "2024-07-22T14:59:19.203315Z"
    },
    "papermill": {
     "duration": 0.080115,
     "end_time": "2024-07-22T14:59:19.207211",
     "exception": false,
     "start_time": "2024-07-22T14:59:19.127096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame with Missing Values:\n",
      "   Rooms  LotSize  YearBuilt     Price\n",
      "0    3.0   5000.0     2000.0  300000.0\n",
      "1    4.0   6000.0     1995.0  400000.0\n",
      "2    NaN   7000.0     2010.0  500000.0\n",
      "3    3.0      NaN     2005.0  350000.0\n",
      "4    5.0  10000.0     2015.0       NaN\n",
      "5    4.0   8000.0     2000.0  450000.0\n",
      "6    NaN   6000.0     2005.0  420000.0\n",
      "7    3.0      NaN     2010.0  390000.0\n",
      "8    4.0   7000.0        NaN  410000.0\n",
      "9    5.0   9000.0     2015.0       NaN\n",
      "\n",
      "DataFrame After MICE Imputation:\n",
      "      Rooms       LotSize    YearBuilt          Price\n",
      "0  3.000000   5000.000000  2000.000000  300000.000000\n",
      "1  4.000000   6000.000000  1995.000000  400000.000000\n",
      "2  4.592029   7000.000000  2010.000000  500000.000000\n",
      "3  3.000000   5566.690549  2005.000000  350000.000000\n",
      "4  5.000000  10000.000000  2015.000000  575717.968173\n",
      "5  4.000000   8000.000000  2000.000000  450000.000000\n",
      "6  3.881060   6000.000000  2005.000000  420000.000000\n",
      "7  3.000000   6253.915951  2010.000000  390000.000000\n",
      "8  4.000000   7000.000000  2004.781337  410000.000000\n",
      "9  5.000000   9000.000000  2015.000000  528230.503555\n"
     ]
    }
   ],
   "source": [
    "#Multivariate Imputation by Chained Equations (MICE) Example\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a realistic sample DataFrame with missing values\n",
    "data = {\n",
    "    'Rooms': [3, 4, np.nan, 3, 5, 4, np.nan, 3, 4, 5],\n",
    "    'LotSize': [5000, 6000, 7000, np.nan, 10000, 8000, 6000, np.nan, 7000, 9000],\n",
    "    'YearBuilt': [2000, 1995, 2010, 2005, 2015, 2000, 2005, 2010, np.nan, 2015],\n",
    "    'Price': [300000, 400000, 500000, 350000, np.nan, 450000, 420000, 390000, 410000, np.nan]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame with Missing Values:\")\n",
    "print(df)\n",
    "\n",
    "#Apply MICE Imputation\n",
    "#We will use the IterativeImputer from sklearn.experimental to perform MICE imputation.\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Initialize the IterativeImputer (MICE)\n",
    "mice_imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "\n",
    "# Apply the MICE imputer to the DataFrame\n",
    "df_imputed = pd.DataFrame(mice_imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"\\nDataFrame After MICE Imputation:\")\n",
    "print(df_imputed)\n",
    "\n",
    "#Iterative Process: The imputer models each feature with missing values as a function of other features and imputes the missing values iteratively.\n",
    "#Convergence: The process repeats for a specified number of iterations or until the imputed values converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d15e1350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:19.243972Z",
     "iopub.status.busy": "2024-07-22T14:59:19.243180Z",
     "iopub.status.idle": "2024-07-22T14:59:19.330551Z",
     "shell.execute_reply": "2024-07-22T14:59:19.328950Z"
    },
    "papermill": {
     "duration": 0.10873,
     "end_time": "2024-07-22T14:59:19.333205",
     "exception": false,
     "start_time": "2024-07-22T14:59:19.224475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame with Missing Values:\n",
      "   Rooms  LotSize  YearBuilt     Price\n",
      "0    3.0   5000.0     2000.0  300000.0\n",
      "1    4.0   6000.0     1995.0  400000.0\n",
      "2    NaN   7000.0     2010.0  500000.0\n",
      "3    3.0      NaN     2005.0  350000.0\n",
      "4    5.0  10000.0     2015.0       NaN\n",
      "5    4.0   8000.0     2000.0  450000.0\n",
      "6    NaN   6000.0     2005.0  420000.0\n",
      "7    3.0      NaN     2010.0  390000.0\n",
      "8    4.0   7000.0        NaN  410000.0\n",
      "9    5.0   9000.0     2015.0       NaN\n",
      "\n",
      "DataFrame After Imputing Missing Rooms:\n",
      "      Rooms  LotSize  YearBuilt     Price\n",
      "0  3.000000   5000.0     2000.0  300000.0\n",
      "1  4.000000   6000.0     1995.0  400000.0\n",
      "2  3.976922   7000.0     2010.0  500000.0\n",
      "3  3.000000      NaN     2005.0  350000.0\n",
      "4  5.000000  10000.0     2015.0       NaN\n",
      "5  4.000000   8000.0     2000.0  450000.0\n",
      "6  3.410109   6000.0     2005.0  420000.0\n",
      "7  3.000000      NaN     2010.0  390000.0\n",
      "8  4.000000   7000.0        NaN  410000.0\n",
      "9  5.000000   9000.0     2015.0       NaN\n",
      "\n",
      "DataFrame After Imputing Missing LotSize:\n",
      "      Rooms       LotSize  YearBuilt     Price\n",
      "0  3.000000   5000.000000     2000.0  300000.0\n",
      "1  4.000000   6000.000000     1995.0  400000.0\n",
      "2  3.976922   7000.000000     2010.0  500000.0\n",
      "3  3.000000   5149.427367     2005.0  350000.0\n",
      "4  5.000000  10000.000000     2015.0       NaN\n",
      "5  4.000000   8000.000000     2000.0  450000.0\n",
      "6  3.410109   6000.000000     2005.0  420000.0\n",
      "7  3.000000   5403.967846     2010.0  390000.0\n",
      "8  4.000000   7000.000000        NaN  410000.0\n",
      "9  5.000000   9000.000000     2015.0       NaN\n",
      "\n",
      "DataFrame After Imputing Missing YearBuilt:\n",
      "      Rooms       LotSize    YearBuilt     Price\n",
      "0  3.000000   5000.000000  2000.000000  300000.0\n",
      "1  4.000000   6000.000000  1995.000000  400000.0\n",
      "2  3.976922   7000.000000  2010.000000  500000.0\n",
      "3  3.000000   5149.427367  2005.000000  350000.0\n",
      "4  5.000000  10000.000000  2015.000000       NaN\n",
      "5  4.000000   8000.000000  2000.000000  450000.0\n",
      "6  3.410109   6000.000000  2005.000000  420000.0\n",
      "7  3.000000   5403.967846  2010.000000  390000.0\n",
      "8  4.000000   7000.000000  2005.879592  410000.0\n",
      "9  5.000000   9000.000000  2015.000000       NaN\n",
      "\n",
      "DataFrame After Imputing Missing Price:\n",
      "      Rooms       LotSize    YearBuilt          Price\n",
      "0  3.000000   5000.000000  2000.000000  300000.000000\n",
      "1  4.000000   6000.000000  1995.000000  400000.000000\n",
      "2  3.976922   7000.000000  2010.000000  500000.000000\n",
      "3  3.000000   5149.427367  2005.000000  350000.000000\n",
      "4  5.000000  10000.000000  2015.000000  633049.977465\n",
      "5  4.000000   8000.000000  2000.000000  450000.000000\n",
      "6  3.410109   6000.000000  2005.000000  420000.000000\n",
      "7  3.000000   5403.967846  2010.000000  390000.000000\n",
      "8  4.000000   7000.000000  2005.879592  410000.000000\n",
      "9  5.000000   9000.000000  2015.000000  617973.115293\n"
     ]
    }
   ],
   "source": [
    "#Regression Imputation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a realistic sample DataFrame with missing values\n",
    "data = {\n",
    "    'Rooms': [3, 4, np.nan, 3, 5, 4, np.nan, 3, 4, 5],\n",
    "    'LotSize': [5000, 6000, 7000, np.nan, 10000, 8000, 6000, np.nan, 7000, 9000],\n",
    "    'YearBuilt': [2000, 1995, 2010, 2005, 2015, 2000, 2005, 2010, np.nan, 2015],\n",
    "    'Price': [300000, 400000, 500000, 350000, np.nan, 450000, 420000, 390000, 410000, np.nan]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame with Missing Values:\")\n",
    "print(df)\n",
    "\n",
    "#Impute Missing Values in Rooms Using LotSize, YearBuilt, and Price\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Initialize SimpleImputer to fill initial missing values with the mean for regression\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Separate features and target for imputation of Rooms\n",
    "features_rooms = df[['LotSize', 'YearBuilt', 'Price']]\n",
    "target_rooms = df['Rooms']\n",
    "\n",
    "# Impute initial missing values for features\n",
    "features_rooms_imputed = pd.DataFrame(imputer.fit_transform(features_rooms), columns=features_rooms.columns)\n",
    "\n",
    "# Data where Rooms is not missing\n",
    "not_missing_rooms = df[df['Rooms'].notna()]\n",
    "missing_rooms = df[df['Rooms'].isna()]\n",
    "\n",
    "# Fit linear regression model to predict Rooms using LotSize, YearBuilt, and Price\n",
    "model_rooms = LinearRegression()\n",
    "model_rooms.fit(features_rooms_imputed.loc[not_missing_rooms.index], not_missing_rooms['Rooms'])\n",
    "\n",
    "# Predict missing values of Rooms\n",
    "predicted_rooms = model_rooms.predict(features_rooms_imputed.loc[missing_rooms.index])\n",
    "\n",
    "# Fill the missing values in Rooms\n",
    "df.loc[df['Rooms'].isna(), 'Rooms'] = predicted_rooms\n",
    "\n",
    "print(\"\\nDataFrame After Imputing Missing Rooms:\")\n",
    "print(df)\n",
    "\n",
    "#Impute Missing Values in LotSize Using Rooms, YearBuilt, and Price\n",
    "# Separate features and target for imputation of LotSize\n",
    "features_lotsize = df[['Rooms', 'YearBuilt', 'Price']]\n",
    "target_lotsize = df['LotSize']\n",
    "\n",
    "# Impute initial missing values for features\n",
    "features_lotsize_imputed = pd.DataFrame(imputer.fit_transform(features_lotsize), columns=features_lotsize.columns)\n",
    "\n",
    "# Data where LotSize is not missing\n",
    "not_missing_lotsize = df[df['LotSize'].notna()]\n",
    "missing_lotsize = df[df['LotSize'].isna()]\n",
    "\n",
    "# Fit linear regression model to predict LotSize using Rooms, YearBuilt, and Price\n",
    "model_lotsize = LinearRegression()\n",
    "model_lotsize.fit(features_lotsize_imputed.loc[not_missing_lotsize.index], not_missing_lotsize['LotSize'])\n",
    "\n",
    "# Predict missing values of LotSize\n",
    "predicted_lotsize = model_lotsize.predict(features_lotsize_imputed.loc[missing_lotsize.index])\n",
    "\n",
    "# Fill the missing values in LotSize\n",
    "df.loc[df['LotSize'].isna(), 'LotSize'] = predicted_lotsize\n",
    "\n",
    "print(\"\\nDataFrame After Imputing Missing LotSize:\")\n",
    "print(df)\n",
    "\n",
    "#Impute Missing Values in YearBuilt Using Rooms, LotSize, and Price\n",
    "# Separate features and target for imputation of YearBuilt\n",
    "features_yearbuilt = df[['Rooms', 'LotSize', 'Price']]\n",
    "target_yearbuilt = df['YearBuilt']\n",
    "\n",
    "# Impute initial missing values for features\n",
    "features_yearbuilt_imputed = pd.DataFrame(imputer.fit_transform(features_yearbuilt), columns=features_yearbuilt.columns)\n",
    "\n",
    "# Data where YearBuilt is not missing\n",
    "not_missing_yearbuilt = df[df['YearBuilt'].notna()]\n",
    "missing_yearbuilt = df[df['YearBuilt'].isna()]\n",
    "\n",
    "# Fit linear regression model to predict YearBuilt using Rooms, LotSize, and Price\n",
    "model_yearbuilt = LinearRegression()\n",
    "model_yearbuilt.fit(features_yearbuilt_imputed.loc[not_missing_yearbuilt.index], not_missing_yearbuilt['YearBuilt'])\n",
    "\n",
    "# Predict missing values of YearBuilt\n",
    "predicted_yearbuilt = model_yearbuilt.predict(features_yearbuilt_imputed.loc[missing_yearbuilt.index])\n",
    "\n",
    "# Fill the missing values in YearBuilt\n",
    "df.loc[df['YearBuilt'].isna(), 'YearBuilt'] = predicted_yearbuilt\n",
    "\n",
    "print(\"\\nDataFrame After Imputing Missing YearBuilt:\")\n",
    "print(df)\n",
    "\n",
    "#Impute Missing Values in Price Using Rooms, LotSize, and YearBuilt\n",
    "# Separate features and target for imputation of Price\n",
    "features_price = df[['Rooms', 'LotSize', 'YearBuilt']]\n",
    "target_price = df['Price']\n",
    "\n",
    "# Impute initial missing values for features\n",
    "features_price_imputed = pd.DataFrame(imputer.fit_transform(features_price), columns=features_price.columns)\n",
    "\n",
    "# Data where Price is not missing\n",
    "not_missing_price = df[df['Price'].notna()]\n",
    "missing_price = df[df['Price'].isna()]\n",
    "\n",
    "# Fit linear regression model to predict Price using Rooms, LotSize, and YearBuilt\n",
    "model_price = LinearRegression()\n",
    "model_price.fit(features_price_imputed.loc[not_missing_price.index], not_missing_price['Price'])\n",
    "\n",
    "# Predict missing values of Price\n",
    "predicted_price = model_price.predict(features_price_imputed.loc[missing_price.index])\n",
    "\n",
    "# Fill the missing values in Price\n",
    "df.loc[df['Price'].isna(), 'Price'] = predicted_price\n",
    "\n",
    "print(\"\\nDataFrame After Imputing Missing Price:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e55141",
   "metadata": {
    "papermill": {
     "duration": 0.017545,
     "end_time": "2024-07-22T14:59:19.368739",
     "exception": false,
     "start_time": "2024-07-22T14:59:19.351194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Mean, Variance & Standard Deviation\n",
    "**Variance Definition**\n",
    "  * Variance measures the average squared deviation of each data point from the mean.\n",
    "  \n",
    "**Standard Deviation Definition**\n",
    "  * Standard deviation is the square root of the variance and provides a measure of the average distance from the mean.\n",
    "  \n",
    "***Significance of Knowing Variance and Standard Deviation for a Feature***\n",
    "1. **Understanding Spread**:\n",
    "\n",
    "  * Variance and standard deviation indicate how much the data points differ from the mean. A higher value means more spread, while a lower value means the data points are closer to the mean.\n",
    "  \n",
    "2. **Detecting Outliers**:\n",
    "\n",
    "  * High variance or standard deviation may indicate the presence of outliers. Outliers can significantly affect the mean and other statistical measures.\n",
    "  \n",
    "3. **Comparing Variability**:\n",
    "\n",
    "  * These metrics allow for the comparison of variability between different datasets or features. For instance, knowing the standard deviation of house prices in different regions can inform about market volatility.\n",
    "  \n",
    "4. **Data Consistency**:\n",
    "\n",
    "  * Low variance or standard deviation suggests that the data points are consistent, which can be desirable in many contexts (e.g., consistent manufacturing process).\n",
    "  \n",
    "5. **Model Assumptions**:\n",
    "\n",
    "  * Many statistical models assume a certain level of variance or standard deviation. Understanding these metrics helps in validating these assumptions.\n",
    "  \n",
    "  \n",
    "***Acceptability of Variance and Standard Deviation***\n",
    "1. **Context-Dependent**:\n",
    "\n",
    "  * In some fields, a high standard deviation might be expected and acceptable, while in others, it might indicate a problem.\n",
    "  * Example: In finance, high variance in stock prices indicates high risk, which might be acceptable for high-reward investments. In manufacturing, high variance in product dimensions might be unacceptable as it indicates inconsistent quality.\n",
    "  \n",
    "2. **Relative Comparison**:\n",
    "\n",
    "  * Compare the variance or standard deviation to historical data, industry standards, or other relevant benchmarks.\n",
    "  * Example: If the standard deviation of delivery times for a logistics company is significantly higher than industry benchmarks, it might indicate inefficiencies.\n",
    "  \n",
    "3. **Domain Knowledge**:\n",
    "\n",
    "  * Use domain knowledge to determine acceptable levels of variability.\n",
    "  * Example: In healthcare, acceptable variability in patient blood pressure readings might be very low, while variability in customer preferences for a retail business might be higher.\n",
    "  \n",
    "***Handling High Variance and Standard Deviation***\n",
    "1. **Data Transformation**:\n",
    "\n",
    "  * Apply transformations to stabilize variance and reduce skewness.\n",
    "  * Example: Log transformation, square root transformation, or Box-Cox transformation can reduce variability in right-skewed data.\n",
    "    \n",
    "    * import numpy as np\n",
    "    * df['Log_House_Price'] = np.log(df['House_Price'])\n",
    "    \n",
    "2. **Outlier Treatment**:\n",
    "  * Identify and handle outliers which can inflate variance.\n",
    "  * Example: Remove or transform outliers.\n",
    "    * df_no_outliers = df[df['House_Price'] < df['House_Price'].quantile(0.95)]\n",
    "    \n",
    "3. **Feature Engineering**:\n",
    "\n",
    "  * Create new features that capture important patterns and reduce variability.\n",
    "  * Example: Create an aggregate feature that combines multiple related features.\n",
    "    * df['Price_Per_Square_Foot'] = df['House_Price'] / df['House_Size']\n",
    "    \n",
    "4. **Normalization and Standardization**:\n",
    "\n",
    "  * Normalize or standardize your data to bring all features onto a similar scale.\n",
    "  * Example: Min-max normalization or z-score standardization.\n",
    "    * from sklearn.preprocessing import StandardScaler\n",
    "    * scaler = StandardScaler()\n",
    "    * df[['House_Price', 'House_Size']] = scaler.fit_transform(df[['House_Price', 'House_Size']])\n",
    "\n",
    "5. **Segmenting Data**:\n",
    "\n",
    "  * Segment your data into more homogeneous groups to reduce variability within each group.\n",
    "  * Example: Segmenting customer data by age group or region.\n",
    "    * df_grouped = df.groupby('Region')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d03a8",
   "metadata": {
    "papermill": {
     "duration": 0.017449,
     "end_time": "2024-07-22T14:59:19.403731",
     "exception": false,
     "start_time": "2024-07-22T14:59:19.386282",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Skewness and Kurtosis\n",
    "  * **Skewness Definition**:\n",
    "    * measures the asymmetry of a distribution. It indicates whether the data points are skewed to the left (negative skewness) or to the right (positive skewness).\n",
    "  * **Interpretation**:\n",
    "    * Negative Skewness (< 0): The left tail is longer; the bulk of the data points are to the right (he majority of the data points are concentrated on the right side).\n",
    "    * Positive Skewness (> 0): The right tail is longer; the bulk of the data points are to the left (The majority of the data points are concentrated on the left side).\n",
    "    * Zero Skewness (≈ 0): The distribution is symmetric.\n",
    "    \n",
    "* **Kurtosis Definition**:\n",
    "  *  measures the \"tailedness\" of the distribution. It indicates whether the data points have heavy or light tails compared to a normal distribution.\n",
    "  * **Interpretation**:\n",
    "    * Leptokurtic (Kurtosis > 0): Distribution has heavier tails and a sharper peak than a normal distribution ( indicating more data points in the tails and a higher peak).\n",
    "    * Platykurtic (Kurtosis < 0): Distribution has lighter tails and a flatter peak than a normal distribution (indicating fewer data points in the tails and a broader, flatter peak).\n",
    "    * Mesokurtic (Kurtosis ≈ 0): Distribution has tails and peak similar to a normal distribution.\n",
    "    \n",
    "    \n",
    "* **Example Output Interpretation**\n",
    "\n",
    "Skewness: 0.5765521555260447 and \n",
    "Kurtosis: -0.9263416062075296\n",
    "\n",
    "  * Skewness: 0.577 (Positive Skewness)\n",
    "\n",
    "    * The distribution is right-skewed, indicating that there are more data points on the lower end of house prices, with some higher values pulling the tail to the right.\n",
    "    \n",
    "  * Kurtosis: -0.926 (Platykurtic)\n",
    "\n",
    "    * The distribution has lighter tails and a flatter peak compared to a normal distribution, indicating less extreme values and a broader peak.\n",
    "\n",
    "* **Why Understanding Skewness and Kurtosis is Important**:\n",
    "  * **Model Assumptions**: Many statistical models assume normally distributed residuals. Understanding skewness and kurtosis helps validate these assumptions.\n",
    "  * **Data Transformation**: If the data is highly skewed or has extreme kurtosis, transformations (e.g., log transformation) might be necessary to meet model assumptions or improve performance.\n",
    "  * **Outlier Detection**: High kurtosis indicates potential outliers, which can be further investigated.\n",
    "  * **Descriptive Analysis**: Skewness and kurtosis provide a deeper understanding of the data's distribution, beyond just mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f398d1",
   "metadata": {
    "papermill": {
     "duration": 0.018702,
     "end_time": "2024-07-22T14:59:19.439967",
     "exception": false,
     "start_time": "2024-07-22T14:59:19.421265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Comparing Variance/Standard Deviation with Skewness\n",
    "**Variance and Standard Deviation**:\n",
    "  * Focus on Spread: They provide information about the dispersion or spread of the data around the mean.\n",
    "  * Use in Dispersion: Essential for understanding the overall variability and consistency of data points.\n",
    "  \n",
    "**Skewness**:\n",
    "  * Focus on Asymmetry: Provides information about the direction and degree of asymmetry in the data.\n",
    "  * Use in Shape: Crucial for understanding the shape of the distribution and identifying potential biases in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a675be",
   "metadata": {
    "papermill": {
     "duration": 0.017415,
     "end_time": "2024-07-22T14:59:19.475150",
     "exception": false,
     "start_time": "2024-07-22T14:59:19.457735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Example Code for understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53be88dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:19.512651Z",
     "iopub.status.busy": "2024-07-22T14:59:19.512202Z",
     "iopub.status.idle": "2024-07-22T14:59:20.094628Z",
     "shell.execute_reply": "2024-07-22T14:59:20.093419Z"
    },
    "papermill": {
     "duration": 0.604859,
     "end_time": "2024-07-22T14:59:20.097572",
     "exception": false,
     "start_time": "2024-07-22T14:59:19.492713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 342250.0\n",
      "Standard Deviation: 88309.78666632953\n",
      "Variance: 7798618421.052631\n",
      "Skewness: 0.0009318739660160223\n",
      "Kurtosis: -2.0788965251549825\n",
      "The distribution is right-skewed.\n",
      "The distribution has lighter tails than a normal distribution (platykurtic).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'House_Price': [250000, 270000, 260000, 280000, 240000, 230000, 255000, 265000, 250000, 275000,\n",
    "                    400000, 450000, 420000, 430000, 410000, 440000, 435000, 425000, 415000, 445000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate statistics\n",
    "mean_price = df['House_Price'].mean()\n",
    "std_dev_price = df['House_Price'].std()\n",
    "variance_price = df['House_Price'].var()\n",
    "skewness_price = df['House_Price'].skew()\n",
    "kurtosis_price = df['House_Price'].kurtosis()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Mean: {mean_price}\")\n",
    "print(f\"Standard Deviation: {std_dev_price}\")\n",
    "print(f\"Variance: {variance_price}\")\n",
    "print(f\"Skewness: {skewness_price}\")\n",
    "print(f\"Kurtosis: {kurtosis_price}\")\n",
    "\n",
    "# Interpretation\n",
    "if skewness_price > 0:\n",
    "    print(\"The distribution is right-skewed.\")\n",
    "elif skewness_price < 0:\n",
    "    print(\"The distribution is left-skewed.\")\n",
    "else:\n",
    "    print(\"The distribution is symmetric.\")\n",
    "\n",
    "if kurtosis_price > 0:\n",
    "    print(\"The distribution has heavier tails than a normal distribution (leptokurtic).\")\n",
    "elif kurtosis_price < 0:\n",
    "    print(\"The distribution has lighter tails than a normal distribution (platykurtic).\")\n",
    "else:\n",
    "    print(\"The distribution has normal tails (mesokurtic).\")\n",
    "\n",
    "# Visualizing the distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df['House_Price'], bins=10, kde=True)\n",
    "plt.title('House Prices Distribution')\n",
    "plt.xlabel('House Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce71a5d",
   "metadata": {
    "papermill": {
     "duration": 0.019298,
     "end_time": "2024-07-22T14:59:20.139579",
     "exception": false,
     "start_time": "2024-07-22T14:59:20.120281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Example Code to identify the Swewness and Kurtosis\n",
    "**Interpreting the Results**\n",
    "1. **Mean**: 342,250.0\n",
    "\n",
    "    * This is the average house price in your dataset.\n",
    "\n",
    "2. **Standard Deviation**: 88,309.79\n",
    "\n",
    "    * This indicates that house prices typically vary by approximately dollar 88,309 from the mean.\n",
    "    \n",
    "3. **Variance**: 7,798,618,421.05\n",
    "\n",
    "    * This is a measure of the dispersion of house prices around the mean.\n",
    "    \n",
    "4. **Skewness**: 0.00093\n",
    "\n",
    "    * The distribution is almost symmetric with a very slight right skew. Generally, a skewness close to 0 indicates a nearly symmetric distribution, so this is typically not a major concern.\n",
    "    \n",
    "5. **Kurtosis**: -2.0789\n",
    "\n",
    "    * The distribution is platykurtic, meaning it has lighter tails and a flatter peak compared to a normal distribution. This indicates fewer extreme values or outliers than a normal distribution.\n",
    "\n",
    "**Do You Need to Take Corrective Action?**\n",
    "1. **Skewness**\n",
    "  * **Slight Right Skew**: Since the skewness is very close to 0, indicating an almost symmetric distribution, there is typically no need for corrective action for skewness.\n",
    "  \n",
    "2. **Kurtosis**\n",
    "  * **Platykurtic Distribution**: This suggests fewer extreme values and a flatter peak. This can be beneficial as it indicates fewer outliers. However, if your specific analysis or model requires normally distributed data, you might consider transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e451e1",
   "metadata": {
    "papermill": {
     "duration": 0.018444,
     "end_time": "2024-07-22T14:59:20.178034",
     "exception": false,
     "start_time": "2024-07-22T14:59:20.159590",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Methods to Identify Outliers\n",
    "1. Interquartile Range (IQR) Method\n",
    "2. Z-Score Method\n",
    "3. Modified Z-Score Method **--> Not a great method**\n",
    "4. Percentile Method\n",
    "5. Isolation Forest\n",
    "6. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)  **--> Not a great method**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d645d9d4",
   "metadata": {
    "papermill": {
     "duration": 0.018131,
     "end_time": "2024-07-22T14:59:20.214661",
     "exception": false,
     "start_time": "2024-07-22T14:59:20.196530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Interquartile Range (IQR) Method\n",
    "**Quartiles** divide a ranked dataset into four equal parts. Each quartile contains 25% of the data. Here's a breakdown of the key quartiles:\n",
    "\n",
    "  * First Quartile (Q1): This is the 25th percentile. It marks the point below which 25% of the data falls.\n",
    "  * Third Quartile (Q3): This is the 75th percentile. It marks the point below which 75% of the data falls.\n",
    "  * Interquartile Range (IQR): The IQR is the range between the first and third quartiles. It measures the spread of the middle 50% of the data.\n",
    "\n",
    "IQR=Q3−Q1\n",
    "Identifying Outliers Using the IQR Method.\n",
    "\n",
    "The IQR method defines outliers as data points that lie outside the range:\n",
    "[𝑄1−1.5×IQR,𝑄3+1.5×IQR]\n",
    "[Q1−1.5×IQR,Q3+1.5×IQR]\n",
    "\n",
    "  * Lower Bound: 𝑄1−1.5×IQR\n",
    "  * Upper Bound: 𝑄3+1.5×IQR\n",
    "\n",
    "**Use When**:\n",
    "\n",
    "  * The data distribution is approximately symmetric.\n",
    "  * You need a simple, easy-to-understand method.\n",
    "  * Your dataset is relatively small or moderately sized.\n",
    "  \n",
    "**Why**:\n",
    "\n",
    "  * IQR is robust to extreme values and does not assume a specific distribution.\n",
    "  * It is easy to implement and interpret.\n",
    "  \n",
    "**Limitations**:\n",
    "\n",
    "  * Not effective for datasets with multimodal distributions or clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9aa73fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:20.254062Z",
     "iopub.status.busy": "2024-07-22T14:59:20.252981Z",
     "iopub.status.idle": "2024-07-22T14:59:20.273450Z",
     "shell.execute_reply": "2024-07-22T14:59:20.271893Z"
    },
    "papermill": {
     "duration": 0.042745,
     "end_time": "2024-07-22T14:59:20.275903",
     "exception": false,
     "start_time": "2024-07-22T14:59:20.233158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Quartile (Q1): 256250.0\n",
      "Third Quartile (Q3): 428750.0\n",
      "Interquartile Range (IQR): 172500.0\n",
      "lower_bound:  -2500.0\n",
      "upper_bound:  687500.0\n",
      "IQR:  172500.0\n",
      "outliers:      House_Price\n",
      "0    -100000000\n",
      "21     55500000\n",
      "Outliers identified by IQR method:\n",
      "    House_Price\n",
      "0    -100000000\n",
      "21     55500000\n",
      "Data without outliers:\n",
      "    House_Price\n",
      "1        250000\n",
      "2        270000\n",
      "3        260000\n",
      "4        280000\n",
      "5        240000\n",
      "6        230000\n",
      "7        255000\n",
      "8        265000\n",
      "9        250000\n",
      "10       275000\n",
      "11       400000\n",
      "12       450000\n",
      "13       420000\n",
      "14       430000\n",
      "15       410000\n",
      "16       440000\n",
      "17       435000\n",
      "18       425000\n",
      "19       415000\n",
      "20       445000\n"
     ]
    }
   ],
   "source": [
    "#Example Calculations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'House_Price': [-100000000,250000, 270000, 260000, 280000, 240000, 230000, 255000, 265000, 250000, 275000,\n",
    "                    400000, 450000, 420000, 430000, 410000, 440000, 435000, 425000, 415000, 445000,55500000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate Q1, Q3, and IQR\n",
    "Q1 = df['House_Price'].quantile(0.25)\n",
    "Q3 = df['House_Price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(f\"First Quartile (Q1): {Q1}\")\n",
    "print(f\"Third Quartile (Q3): {Q3}\")\n",
    "print(f\"Interquartile Range (IQR): {IQR}\")\n",
    "\n",
    "#Identify Outliers\n",
    "# Calculate bounds for outliers\n",
    "lower_bound = Q1 - (1.5 * IQR)\n",
    "\n",
    "upper_bound = Q3 + (1.5 * IQR)\n",
    "print (\"lower_bound: \",lower_bound)\n",
    "print (\"upper_bound: \",upper_bound)\n",
    "print (\"IQR: \",IQR)\n",
    "\n",
    "# Identify outliers\n",
    "outliers = df[(df['House_Price'] < lower_bound) | (df['House_Price'] > upper_bound)]\n",
    "print (\"outliers: \", outliers)\n",
    "\n",
    "print(\"Outliers identified by IQR method:\")\n",
    "print(outliers)\n",
    "# Remove outliers\n",
    "df_no_outliers = df[(df['House_Price'] >= lower_bound) & (df['House_Price'] <= upper_bound)]\n",
    "\n",
    "print(\"Data without outliers:\")\n",
    "print(df_no_outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b121463",
   "metadata": {
    "papermill": {
     "duration": 0.018345,
     "end_time": "2024-07-22T14:59:20.312918",
     "exception": false,
     "start_time": "2024-07-22T14:59:20.294573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Z-Score Method\n",
    "The **Z-score method** is a statistical technique used to identify outliers in a dataset. It measures how many standard deviations a data point is from the mean of the dataset. If the Z-score of a data point is very high or very low (typically greater than 3 or less than -3), it is considered an outlier.\n",
    "\n",
    "**Use When**:\n",
    "\n",
    "  * The data follows a normal distribution or is approximately symmetric.\n",
    "  * You have a large dataset.\n",
    "  * You want to identify outliers based on statistical properties.\n",
    "  \n",
    "**Why**:\n",
    "\n",
    "  * Z-score provides a standardized way to identify outliers based on the number of standard deviations from the mean.\n",
    "  \n",
    "**Limitations**:\n",
    "\n",
    "  * Sensitive to extreme values and assumes the data is normally distributed.\n",
    "  * Not suitable for datasets with skewed distributions or clusters.\n",
    "\n",
    "**Example Calculation**\n",
    "\n",
    "Let's use the sample dataset again to demonstrate the Z-score method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd7cb1db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:20.353059Z",
     "iopub.status.busy": "2024-07-22T14:59:20.352141Z",
     "iopub.status.idle": "2024-07-22T14:59:20.372719Z",
     "shell.execute_reply": "2024-07-22T14:59:20.371645Z"
    },
    "papermill": {
     "duration": 0.043896,
     "end_time": "2024-07-22T14:59:20.375964",
     "exception": false,
     "start_time": "2024-07-22T14:59:20.332068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   House_Price\n",
      "0   -100000000\n",
      "1       250000\n",
      "2       270000\n",
      "3       260000\n",
      "4       280000\n",
      "With Z Score DF:\n",
      "   House_Price  Z_Score_House_Price\n",
      "0   -100000000            -1.029016\n",
      "1       250000            -0.174809\n",
      "2       270000            -0.174639\n",
      "3       260000            -0.174724\n",
      "4       280000            -0.174554\n",
      "Outliers identified by Z-score method:\n",
      "    House_Price  Z_Score_House_Price\n",
      "21    550000000             4.509483\n",
      "Data without outliers:\n",
      "    House_Price  Z_Score_House_Price\n",
      "0    -100000000            -1.029016\n",
      "1        250000            -0.174809\n",
      "2        270000            -0.174639\n",
      "3        260000            -0.174724\n",
      "4        280000            -0.174554\n",
      "5        240000            -0.174895\n",
      "6        230000            -0.174980\n",
      "7        255000            -0.174767\n",
      "8        265000            -0.174682\n",
      "9        250000            -0.174809\n",
      "10       275000            -0.174596\n",
      "11       400000            -0.173531\n",
      "12       450000            -0.173105\n",
      "13       420000            -0.173361\n",
      "14       430000            -0.173276\n",
      "15       410000            -0.173446\n",
      "16       440000            -0.173190\n",
      "17       435000            -0.173233\n",
      "18       425000            -0.173318\n",
      "19       415000            -0.173403\n",
      "20       445000            -0.173148\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Sample DataFrame\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'House_Price': [-100000000, 250000, 270000, 260000, 280000, 240000, 230000, 255000, 265000, 250000, 275000,\n",
    "                    400000, 450000, 420000, 430000, 410000, 440000, 435000, 425000, 415000, 445000,550000000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print (df.head())\n",
    "# Calculate Z-scores\n",
    "df['Z_Score_House_Price'] = zscore(df['House_Price'])\n",
    "print (\"With Z Score DF:\")\n",
    "print (df.head())\n",
    "# Define the threshold\n",
    "threshold = 3.5\n",
    "\n",
    "# Identify outliers\n",
    "outliers = df[(df['Z_Score_House_Price'].abs() > threshold)]\n",
    "\n",
    "print(\"Outliers identified by Z-score method:\")\n",
    "print(outliers)\n",
    "\n",
    "# Remove outliers\n",
    "df_no_outliers = df[(df['Z_Score_House_Price'].abs() <= threshold)]\n",
    "\n",
    "print(\"Data without outliers:\")\n",
    "print(df_no_outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e3c668",
   "metadata": {
    "papermill": {
     "duration": 0.024327,
     "end_time": "2024-07-22T14:59:20.419195",
     "exception": false,
     "start_time": "2024-07-22T14:59:20.394868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Percentile Method\n",
    "The Percentile Method is a non-parametric technique used to identify outliers based on specific percentiles of the data distribution. This method involves setting upper and lower bounds based on chosen percentiles, and any data points outside these bounds are considered outliers.\n",
    "\n",
    "**Key Concepts**\n",
    "  1. **Percentile**: A value below which a given percentage of observations in a dataset falls.\n",
    "  2. **Lower Percentile**: Typically set at the 1st or 5th percentile.\n",
    "  3. **Upper Percentile**: Typically set at the 99th or 95th percentile.\n",
    "  \n",
    "**Steps to Implement the Percentile Method**\n",
    "\n",
    "  1. Calculate the lower percentile (e.g., 1st or 5th percentile).\n",
    "  2. Calculate the upper percentile (e.g., 99th or 95th percentile).\n",
    "  3. Define outliers as data points below the lower percentile or above the upper percentile.\n",
    "  \n",
    "**Use When**:\n",
    "\n",
    "  * You need a simple method to identify outliers based on rank.\n",
    "  * The data distribution is not necessarily normal or symmetric.\n",
    "  \n",
    "**Why**:\n",
    "\n",
    "  * Percentile method is easy to understand and implement.\n",
    "  * It does not assume any specific data distribution.\n",
    "  \n",
    "**Limitations**:\n",
    "\n",
    "  * The choice of percentile thresholds can be arbitrary.\n",
    "  * May not effectively handle multimodal distributions or clusters.\n",
    "  \n",
    "**Example Calculation**\n",
    "\n",
    "Let’s demonstrate the Percentile Method using your sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b88f0a0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:20.466558Z",
     "iopub.status.busy": "2024-07-22T14:59:20.466116Z",
     "iopub.status.idle": "2024-07-22T14:59:20.482718Z",
     "shell.execute_reply": "2024-07-22T14:59:20.481584Z"
    },
    "papermill": {
     "duration": 0.043617,
     "end_time": "2024-07-22T14:59:20.485685",
     "exception": false,
     "start_time": "2024-07-22T14:59:20.442068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Percentile: -78951700.0\n",
      "99th Percentile: 434594499.9999995\n",
      "Outliers identified by Percentile method:\n",
      "    House_Price\n",
      "0    -100000000\n",
      "21    550000000\n",
      "Data without outliers:\n",
      "    House_Price\n",
      "1        250000\n",
      "2        270000\n",
      "3        260000\n",
      "4        280000\n",
      "5        240000\n",
      "6        230000\n",
      "7        255000\n",
      "8        265000\n",
      "9        250000\n",
      "10       275000\n",
      "11       400000\n",
      "12       450000\n",
      "13       420000\n",
      "14       430000\n",
      "15       410000\n",
      "16       440000\n",
      "17       435000\n",
      "18       425000\n",
      "19       415000\n",
      "20       445000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'House_Price': [-100000000, 250000, 270000, 260000, 280000, 240000, 230000, 255000, 265000, 250000, 275000,\n",
    "                    400000, 450000, 420000, 430000, 410000, 440000, 435000, 425000, 415000, 445000, 550000000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate lower and upper percentiles\n",
    "lower_percentile = df['House_Price'].quantile(0.01)  # 1st percentile\n",
    "upper_percentile = df['House_Price'].quantile(0.99)  # 99th percentile\n",
    "\n",
    "print(f\"1st Percentile: {lower_percentile}\")\n",
    "print(f\"99th Percentile: {upper_percentile}\")\n",
    "# Identify outliers\n",
    "outliers = df[(df['House_Price'] < lower_percentile) | (df['House_Price'] > upper_percentile)]\n",
    "\n",
    "print(\"Outliers identified by Percentile method:\")\n",
    "print(outliers)\n",
    "# Remove outliers\n",
    "df_no_outliers = df[(df['House_Price'] >= lower_percentile) & (df['House_Price'] <= upper_percentile)]\n",
    "\n",
    "print(\"Data without outliers:\")\n",
    "print(df_no_outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eef5dd",
   "metadata": {
    "papermill": {
     "duration": 0.018525,
     "end_time": "2024-07-22T14:59:20.523068",
     "exception": false,
     "start_time": "2024-07-22T14:59:20.504543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Isolation Forest\n",
    "**Isolation Forest** is an anomaly detection algorithm that isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. The idea is that anomalies are few and different, thus they are easier to isolate.\n",
    "\n",
    "**Key Concepts**\n",
    "  1. **Isolation**: Anomalies are isolated closer to the root of the tree as they require fewer splits to be isolated compared to normal observations.\n",
    "  2. **Path Length**: The number of edges an observation traverses in a tree before it is isolated.\n",
    "  3. **Anomaly Score**: Based on the path length. Observations with shorter paths are more likely to be anomalies.\n",
    "\n",
    "**Steps to Implement Isolation Forest**\n",
    "  1. **Train the Isolation Forest Model**: Fit the model on the dataset.\n",
    "  2. **Compute Anomaly Scores**: Calculate the anomaly score for each observation.\n",
    "  3. **Set a Contamination Level**: The proportion of the dataset expected to be anomalies.\n",
    "  4. **Identify Anomalies**: Based on the anomaly scores and the contamination level.\n",
    "  \n",
    "**Use When**:\n",
    "\n",
    "  * You have a large, high-dimensional dataset.\n",
    "  * The data may have complex relationships or clusters.\n",
    "  * You need an unsupervised method that does not assume a specific distribution.\n",
    "  \n",
    "**Why**:\n",
    "\n",
    "  * Isolation Forest is effective for high-dimensional data and can handle complex data structures.\n",
    "  * It works well for both continuous and categorical data.\n",
    "  \n",
    "**Limitations**:\n",
    "\n",
    "  * Requires parameter tuning (contamination level, number of estimators).\n",
    "  * Can be computationally expensive for very large datasets.Use When:\n",
    "\n",
    "Example Calculation\n",
    "Let's demonstrate the Isolation Forest method using your sample dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad1292cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:20.641995Z",
     "iopub.status.busy": "2024-07-22T14:59:20.641074Z",
     "iopub.status.idle": "2024-07-22T14:59:21.134705Z",
     "shell.execute_reply": "2024-07-22T14:59:21.133145Z"
    },
    "papermill": {
     "duration": 0.594078,
     "end_time": "2024-07-22T14:59:21.137313",
     "exception": false,
     "start_time": "2024-07-22T14:59:20.543235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers identified by Isolation Forest method:\n",
      "    House_Price  Anomaly  Anomaly_Score\n",
      "0    -100000000       -1      -0.353614\n",
      "6        230000       -1      -0.003408\n",
      "21     55500000       -1      -0.318348\n",
      "Data without outliers:\n",
      "    House_Price  Anomaly  Anomaly_Score\n",
      "1        250000        1       0.097123\n",
      "2        270000        1       0.092183\n",
      "3        260000        1       0.107746\n",
      "4        280000        1       0.045427\n",
      "5        240000        1       0.059042\n",
      "7        255000        1       0.102129\n",
      "8        265000        1       0.103300\n",
      "9        250000        1       0.097123\n",
      "10       275000        1       0.080056\n",
      "11       400000        1       0.045359\n",
      "12       450000        1       0.030673\n",
      "13       420000        1       0.092977\n",
      "14       430000        1       0.094224\n",
      "15       410000        1       0.072277\n",
      "16       440000        1       0.080160\n",
      "17       435000        1       0.088281\n",
      "18       425000        1       0.094017\n",
      "19       415000        1       0.089057\n",
      "20       445000        1       0.068248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Example Calculations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Sample DataFrame\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'House_Price': [-100000000,250000, 270000, 260000, 280000, 240000, 230000, 255000, 265000, 250000, 275000,\n",
    "                    400000, 450000, 420000, 430000, 410000, 440000, 435000, 425000, 415000, 445000,55500000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "# Fit the model\n",
    "iso = IsolationForest(contamination=0.1, random_state=42)\n",
    "df['Anomaly'] = iso.fit_predict(df[['House_Price']])\n",
    "df['Anomaly_Score'] = iso.decision_function(df[['House_Price']])\n",
    "\n",
    "# Identify anomalies\n",
    "outliers = df[df['Anomaly'] == -1]\n",
    "\n",
    "print(\"Outliers identified by Isolation Forest method:\")\n",
    "print(outliers)\n",
    "# Remove outliers\n",
    "df_no_outliers = df[df['Anomaly'] != -1]\n",
    "\n",
    "print(\"Data without outliers:\")\n",
    "print(df_no_outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3853c7db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:21.178622Z",
     "iopub.status.busy": "2024-07-22T14:59:21.178199Z",
     "iopub.status.idle": "2024-07-22T14:59:21.272487Z",
     "shell.execute_reply": "2024-07-22T14:59:21.271382Z"
    },
    "papermill": {
     "duration": 0.119435,
     "end_time": "2024-07-22T14:59:21.275961",
     "exception": false,
     "start_time": "2024-07-22T14:59:21.156526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame with Missing Values:\n",
      "   Rooms  LotSize  YearBuilt     Price\n",
      "0    3.0   5000.0     2000.0  300000.0\n",
      "1    4.0   6000.0     1995.0  400000.0\n",
      "2    NaN   7000.0     2010.0  500000.0\n",
      "3    3.0      NaN     2005.0  350000.0\n",
      "4    5.0  10000.0     2015.0       NaN\n",
      "5    4.0   8000.0     2000.0  450000.0\n",
      "6    NaN   6000.0     2005.0  420000.0\n",
      "7    3.0      NaN     2010.0  390000.0\n",
      "8    4.0   7000.0        NaN  410000.0\n",
      "9    5.0   9000.0     2015.0       NaN\n",
      "\n",
      "DataFrame After Stochastic Regression Imputation for Rooms:\n",
      "      Rooms  LotSize  YearBuilt     Price\n",
      "0  3.000000   5000.0     2000.0  300000.0\n",
      "1  4.000000   6000.0     1995.0  400000.0\n",
      "2  4.222487   7000.0     2010.0  500000.0\n",
      "3  3.000000      NaN     2005.0  350000.0\n",
      "4  5.000000  10000.0     2015.0       NaN\n",
      "5  4.000000   8000.0     2000.0  450000.0\n",
      "6  3.341753   6000.0     2005.0  420000.0\n",
      "7  3.000000      NaN     2010.0  390000.0\n",
      "8  4.000000   7000.0        NaN  410000.0\n",
      "9  5.000000   9000.0     2015.0       NaN\n",
      "\n",
      "DataFrame After Stochastic Regression Imputation for LotSize:\n",
      "      Rooms       LotSize  YearBuilt     Price\n",
      "0  3.000000   5000.000000     2000.0  300000.0\n",
      "1  4.000000   6000.000000     1995.0  400000.0\n",
      "2  4.222487   7000.000000     2010.0  500000.0\n",
      "3  3.000000   5607.218714     2005.0  350000.0\n",
      "4  5.000000  10000.000000     2015.0       NaN\n",
      "5  4.000000   8000.000000     2000.0  450000.0\n",
      "6  3.341753   6000.000000     2005.0  420000.0\n",
      "7  3.000000   6300.999737     2010.0  390000.0\n",
      "8  4.000000   7000.000000        NaN  410000.0\n",
      "9  5.000000   9000.000000     2015.0       NaN\n",
      "\n",
      "DataFrame After Stochastic Regression Imputation for YearBuilt:\n",
      "      Rooms       LotSize    YearBuilt     Price\n",
      "0  3.000000   5000.000000  2000.000000  300000.0\n",
      "1  4.000000   6000.000000  1995.000000  400000.0\n",
      "2  4.222487   7000.000000  2010.000000  500000.0\n",
      "3  3.000000   5607.218714  2005.000000  350000.0\n",
      "4  5.000000  10000.000000  2015.000000       NaN\n",
      "5  4.000000   8000.000000  2000.000000  450000.0\n",
      "6  3.341753   6000.000000  2005.000000  420000.0\n",
      "7  3.000000   6300.999737  2010.000000  390000.0\n",
      "8  4.000000   7000.000000  2004.416576  410000.0\n",
      "9  5.000000   9000.000000  2015.000000       NaN\n",
      "\n",
      "DataFrame After Stochastic Regression Imputation for Price:\n",
      "      Rooms       LotSize    YearBuilt          Price\n",
      "0  3.000000   5000.000000  2000.000000  300000.000000\n",
      "1  4.000000   6000.000000  1995.000000  400000.000000\n",
      "2  4.222487   7000.000000  2010.000000  500000.000000\n",
      "3  3.000000   5607.218714  2005.000000  350000.000000\n",
      "4  5.000000  10000.000000  2015.000000  620803.857904\n",
      "5  4.000000   8000.000000  2000.000000  450000.000000\n",
      "6  3.341753   6000.000000  2005.000000  420000.000000\n",
      "7  3.000000   6300.999737  2010.000000  390000.000000\n",
      "8  4.000000   7000.000000  2004.416576  410000.000000\n",
      "9  5.000000   9000.000000  2015.000000  641307.903275\n"
     ]
    }
   ],
   "source": [
    "#Stochastic Regression Imputation:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a realistic sample DataFrame with missing values\n",
    "data = {\n",
    "    'Rooms': [3, 4, np.nan, 3, 5, 4, np.nan, 3, 4, 5],\n",
    "    'LotSize': [5000, 6000, 7000, np.nan, 10000, 8000, 6000, np.nan, 7000, 9000],\n",
    "    'YearBuilt': [2000, 1995, 2010, 2005, 2015, 2000, 2005, 2010, np.nan, 2015],\n",
    "    'Price': [300000, 400000, 500000, 350000, np.nan, 450000, 420000, 390000, 410000, np.nan]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame with Missing Values:\")\n",
    "print(df)\n",
    "\n",
    "\n",
    "#Apply Stochastic Regression Imputation\n",
    "#We'll use linear regression to predict the missing values and then add a random error term to the predictions.\n",
    "#Impute Missing Values in Rooms Using LotSize, YearBuilt, and Price\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Initialize SimpleImputer to fill initial missing values with the mean for regression\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Separate features and target for imputation of Rooms\n",
    "features_rooms = df[['LotSize', 'YearBuilt', 'Price']]\n",
    "target_rooms = df['Rooms']\n",
    "\n",
    "# Impute initial missing values for features\n",
    "features_rooms_imputed = pd.DataFrame(imputer.fit_transform(features_rooms), columns=features_rooms.columns)\n",
    "\n",
    "# Data where Rooms is not missing\n",
    "not_missing_rooms = df[df['Rooms'].notna()]\n",
    "missing_rooms = df[df['Rooms'].isna()]\n",
    "\n",
    "# Fit linear regression model to predict Rooms using LotSize, YearBuilt, and Price\n",
    "model_rooms = LinearRegression()\n",
    "model_rooms.fit(features_rooms_imputed.loc[not_missing_rooms.index], not_missing_rooms['Rooms'])\n",
    "\n",
    "# Predict missing values of Rooms\n",
    "predicted_rooms = model_rooms.predict(features_rooms_imputed.loc[missing_rooms.index])\n",
    "\n",
    "# Calculate the residual standard deviation\n",
    "residuals_rooms = not_missing_rooms['Rooms'] - model_rooms.predict(features_rooms_imputed.loc[not_missing_rooms.index])\n",
    "std_dev_rooms = np.std(residuals_rooms)\n",
    "\n",
    "# Add random error term to predictions\n",
    "random_error_rooms = np.random.normal(0, std_dev_rooms, size=predicted_rooms.shape)\n",
    "stochastic_predicted_rooms = predicted_rooms + random_error_rooms\n",
    "\n",
    "# Fill the missing values in Rooms\n",
    "df.loc[df['Rooms'].isna(), 'Rooms'] = stochastic_predicted_rooms\n",
    "\n",
    "print(\"\\nDataFrame After Stochastic Regression Imputation for Rooms:\")\n",
    "print(df)\n",
    "\n",
    "#Impute Missing Values in LotSize Using Rooms, YearBuilt, and Price\n",
    "# Separate features and target for imputation of LotSize\n",
    "features_lotsize = df[['Rooms', 'YearBuilt', 'Price']]\n",
    "target_lotsize = df['LotSize']\n",
    "\n",
    "# Impute initial missing values for features\n",
    "features_lotsize_imputed = pd.DataFrame(imputer.fit_transform(features_lotsize), columns=features_lotsize.columns)\n",
    "\n",
    "# Data where LotSize is not missing\n",
    "not_missing_lotsize = df[df['LotSize'].notna()]\n",
    "missing_lotsize = df[df['LotSize'].isna()]\n",
    "\n",
    "# Fit linear regression model to predict LotSize using Rooms, YearBuilt, and Price\n",
    "model_lotsize = LinearRegression()\n",
    "model_lotsize.fit(features_lotsize_imputed.loc[not_missing_lotsize.index], not_missing_lotsize['LotSize'])\n",
    "\n",
    "# Predict missing values of LotSize\n",
    "predicted_lotsize = model_lotsize.predict(features_lotsize_imputed.loc[missing_lotsize.index])\n",
    "\n",
    "# Calculate the residual standard deviation\n",
    "residuals_lotsize = not_missing_lotsize['LotSize'] - model_lotsize.predict(features_lotsize_imputed.loc[not_missing_lotsize.index])\n",
    "std_dev_lotsize = np.std(residuals_lotsize)\n",
    "\n",
    "# Add random error term to predictions\n",
    "random_error_lotsize = np.random.normal(0, std_dev_lotsize, size=predicted_lotsize.shape)\n",
    "stochastic_predicted_lotsize = predicted_lotsize + random_error_lotsize\n",
    "\n",
    "# Fill the missing values in LotSize\n",
    "df.loc[df['LotSize'].isna(), 'LotSize'] = stochastic_predicted_lotsize\n",
    "\n",
    "print(\"\\nDataFrame After Stochastic Regression Imputation for LotSize:\")\n",
    "print(df)\n",
    "\n",
    "#Impute Missing Values in YearBuilt Using Rooms, LotSize, and Price\n",
    "\n",
    "# Separate features and target for imputation of YearBuilt\n",
    "features_yearbuilt = df[['Rooms', 'LotSize', 'Price']]\n",
    "target_yearbuilt = df['YearBuilt']\n",
    "\n",
    "# Impute initial missing values for features\n",
    "features_yearbuilt_imputed = pd.DataFrame(imputer.fit_transform(features_yearbuilt), columns=features_yearbuilt.columns)\n",
    "\n",
    "# Data where YearBuilt is not missing\n",
    "not_missing_yearbuilt = df[df['YearBuilt'].notna()]\n",
    "missing_yearbuilt = df[df['YearBuilt'].isna()]\n",
    "\n",
    "# Fit linear regression model to predict YearBuilt using Rooms, LotSize, and Price\n",
    "model_yearbuilt = LinearRegression()\n",
    "model_yearbuilt.fit(features_yearbuilt_imputed.loc[not_missing_yearbuilt.index], not_missing_yearbuilt['YearBuilt'])\n",
    "\n",
    "# Predict missing values of YearBuilt\n",
    "predicted_yearbuilt = model_yearbuilt.predict(features_yearbuilt_imputed.loc[missing_yearbuilt.index])\n",
    "\n",
    "# Calculate the residual standard deviation\n",
    "residuals_yearbuilt = not_missing_yearbuilt['YearBuilt'] - model_yearbuilt.predict(features_yearbuilt_imputed.loc[not_missing_yearbuilt.index])\n",
    "std_dev_yearbuilt = np.std(residuals_yearbuilt)\n",
    "\n",
    "# Add random error term to predictions\n",
    "random_error_yearbuilt = np.random.normal(0, std_dev_yearbuilt, size=predicted_yearbuilt.shape)\n",
    "stochastic_predicted_yearbuilt = predicted_yearbuilt + random_error_yearbuilt\n",
    "\n",
    "# Fill the missing values in YearBuilt\n",
    "df.loc[df['YearBuilt'].isna(), 'YearBuilt'] = stochastic_predicted_yearbuilt\n",
    "\n",
    "print(\"\\nDataFrame After Stochastic Regression Imputation for YearBuilt:\")\n",
    "print(df)\n",
    "\n",
    "#Impute Missing Values in Price Using Rooms, LotSize, and YearBuilt\n",
    "# Separate features and target for imputation of Price\n",
    "features_price = df[['Rooms', 'LotSize', 'YearBuilt']]\n",
    "target_price = df['Price']\n",
    "\n",
    "# Impute initial missing values for features\n",
    "features_price_imputed = pd.DataFrame(imputer.fit_transform(features_price), columns=features_price.columns)\n",
    "\n",
    "# Data where Price is not missing\n",
    "not_missing_price = df[df['Price'].notna()]\n",
    "missing_price = df[df['Price'].isna()]\n",
    "\n",
    "# Fit linear regression model to predict Price using Rooms, LotSize, and YearBuilt\n",
    "model_price = LinearRegression()\n",
    "model_price.fit(features_price_imputed.loc[not_missing_price.index], not_missing_price['Price'])\n",
    "\n",
    "# Predict missing values of Price\n",
    "predicted_price = model_price.predict(features_price_imputed.loc[missing_price.index])\n",
    "\n",
    "# Calculate the residual standard deviation\n",
    "residuals_price = not_missing_price['Price'] - model_price.predict(features_price_imputed.loc[not_missing_price.index])\n",
    "std_dev_price = np.std(residuals_price)\n",
    "\n",
    "# Add random error term to predictions\n",
    "random_error_price = np.random.normal(0, std_dev_price, size=predicted_price.shape)\n",
    "stochastic_predicted_price = predicted_price + random_error_price\n",
    "\n",
    "# Fill the missing values in Price\n",
    "df.loc[df['Price'].isna(), 'Price'] = stochastic_predicted_price\n",
    "\n",
    "print(\"\\nDataFrame After Stochastic Regression Imputation for Price:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a59d5d",
   "metadata": {
    "papermill": {
     "duration": 0.019769,
     "end_time": "2024-07-22T14:59:21.315422",
     "exception": false,
     "start_time": "2024-07-22T14:59:21.295653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Visualization Of Features and Its Importance \n",
    "Visualizing data is a crucial step in data analysis. It helps in understanding the underlying patterns, distributions, relationships, and anomalies within the data. Different types of plots serve different purposes. Here’s a guide on when to use various plots, why they are needed, and how they help in data analysis.\n",
    "\n",
    "**Summary of When and Why to Use Various Plots**\n",
    "  * **Histograms**: For understanding the distribution of a single variable.\n",
    "  * **Box Plots**: For summarizing distribution and detecting outliers.\n",
    "  * **Scatter Plots**: For examining relationships between two continuous variables.\n",
    "  * **Pair Plots**: For exploring pairwise relationships in the dataset.\n",
    "  * **Heatmaps**: For visualizing correlations and patterns in categorical data.\n",
    "  * **Violin Plots**: For detailed distribution comparison across categories.\n",
    "  * **Line Plots**: For analyzing trends over time.\n",
    "  \n",
    "**Visualizing data using these various plots helps to**\n",
    "\n",
    "  * **Understand Distribution**: Histograms and box plots reveal the shape and spread of data.\n",
    "  * **Identify Relationships**: Scatter plots and pair plots show how variables interact.\n",
    "  * **Detect Outliers**: Box plots and scatter plots can highlight unusual data points.\n",
    "  * **Compare Groups**: Box plots, violin plots, and heatmaps allow for comparing distributions across categories.\n",
    "  * **Analyze Trends**: Line plots are crucial for time series analysis.\n",
    "  \n",
    "**Skewness and Kurtosis**:\n",
    "While skewness and kurtosis provide numerical measures of the distribution's asymmetry and tailedness, visualizing the data with a histogram is still crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe131cc",
   "metadata": {
    "papermill": {
     "duration": 0.018891,
     "end_time": "2024-07-22T14:59:21.354108",
     "exception": false,
     "start_time": "2024-07-22T14:59:21.335217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Correlations & Covariance\n",
    "\n",
    "\n",
    "**Correlation**:  is a statistical measure that describes the extent to which two variables are linearly related. It provides insights into the direction and strength of the relationship between variables. The correlation coefficient ranges from -1 to 1, where:\n",
    "\n",
    "  * +1 indicates a perfect positive linear relationship.\n",
    "  * -1 indicates a perfect negative linear relationship.\n",
    "  * 0 indicates no linear relationship.\n",
    "\n",
    "**Importance of Correlation**\n",
    "  1. **Feature Selection**               : Identifying highly correlated features helps in selecting relevant features for predictive modeling.\n",
    "  2. **Multicollinearity Detection**     : In regression analysis, high correlation between independent variables (multicollinearity) can affect model stability and interpretation.\n",
    "  3. **Data Imputation**                 : Correlation helps in choosing suitable features for imputing missing values.\n",
    "  4. **Exploratory Data Analysis (EDA)** : Correlation analysis is a key step in EDA to understand relationships and patterns in the data.  \n",
    "  5. it's generally advisable to check correlations **after handling missing data**\n",
    "\n",
    "**Why Handle Missing Data First?**\n",
    "  1. **Accuracy**    : Missing data can distort correlation calculations. Including missing values can lead to biased or misleading results.\n",
    "  2. **Completeness**: Handling missing data ensures that the dataset used for correlation analysis is complete, providing more reliable insights.\n",
    "  3. **Consistency** : Different methods of handling missing data (e.g., imputation, deletion) can impact the resulting correlations. Ensuring a     consistent approach helps in making valid comparisons.\n",
    "\n",
    "***Types of Correlation Coefficients***:\n",
    "  1. **Pearson Correlation**       : Measures the linear relationship between two continuous variables. It is sensitive to outliers.\n",
    "  2. **Spearman Rank Correlation** : Measures the monotonic relationship between two variables using their ranks. It is less sensitive to outliers and suitable for ordinal data.\n",
    "  3. **Kendall Tau Correlation**   : Measures the strength of association between two variables and is less sensitive to outliers than Pearson.\n",
    "  \n",
    "  \n",
    "**Covariance**:  is a measure that indicates the extent to which two variables change together. It can be used to determine whether an increase in one variable corresponds to an increase (positive covariance) or decrease (negative covariance) in another variable. A zero covariance indicates no linear relationship between the variables.\n",
    "\n",
    "  1. **Positive Covariance**:  Indicates that as one variable increases, the other variable also increases.\n",
    "  2. **Negative Covariance**: : Indicates that as one variable increases, the other variable decreases.\n",
    "  3. **Zero Covariance**:   3. **Zero Covariance**:  Indicates that there is no linear relationship between the variables.Indicates no linear relationship.\n",
    "\n",
    "  \n",
    "**Summary**\n",
    "  * **Correlation**: Best for understanding linear relationships between continuous variables. Pearson, Spearman, and Kendall are common methods.\n",
    "  * **Covariance**: Indicates how much two variables change together but is scale-dependent.\n",
    "  * **Scenario Analysis**: Helps tailor the approach based on data type and relationship nature.\n",
    "  * **Visualization**: Aids in interpreting complex relationships.\n",
    "  \n",
    "  \n",
    "**Where and Why Correlation useful in Machine Learning**\n",
    "  1. **Exploratory Data Analysis (EDA)**\n",
    "    * When: At the initial stage of ML projects.\n",
    "    * Where: Any dataset where understanding the relationships between features is crucial.\n",
    "    * Why: Helps identify patterns, relationships, and dependencies between features, guiding further analysis and preprocessing steps.\n",
    "    \n",
    "    \n",
    "  2. **Feature Selection**\n",
    "    * When: Before training ML models.\n",
    "    * Where: In datasets with multiple features to select the most relevant ones.\n",
    "    * Why: Helps identify and select features that are strongly related to the target variable, improving model performance and interpretability.\n",
    "    \n",
    "    \n",
    "  3. **Detecting Multicollinearity**\n",
    "    * When: When building regression models or other models sensitive to correlated features.\n",
    "    * Where: In datasets where multiple features might be highly correlated.\n",
    "    * Why: Helps identify and address multicollinearity, which can affect model stability and coefficients in regression models.\n",
    "    \n",
    "    \n",
    "  4. **Data Imputation**\n",
    "    * When: During data preprocessing.\n",
    "    * Where: In datasets with missing values.\n",
    "    * Why: Correlation can guide imputation strategies by identifying related features to use for imputing missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d643307",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:21.394452Z",
     "iopub.status.busy": "2024-07-22T14:59:21.394030Z",
     "iopub.status.idle": "2024-07-22T14:59:21.418187Z",
     "shell.execute_reply": "2024-07-22T14:59:21.417182Z"
    },
    "papermill": {
     "duration": 0.047604,
     "end_time": "2024-07-22T14:59:21.420901",
     "exception": false,
     "start_time": "2024-07-22T14:59:21.373297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Matrix:\n",
      "          Feature1  Feature2\n",
      "Feature1    1.0000    0.7529\n",
      "Feature2    0.7529    1.0000\n",
      "\n",
      "Sample DataFrame:\n",
      "   Feature1  Feature2\n",
      "0  0.496714 -0.225926\n",
      "1 -0.138264 -0.181499\n",
      "2  0.647689  0.156261\n",
      "3  1.523030  0.368529\n",
      "4 -0.234153 -0.142047\n",
      "\n",
      "Spearman Correlation Matrix:\n",
      "             Exam_Rank  Study_Hours\n",
      "Exam_Rank          1.0         -1.0\n",
      "Study_Hours       -1.0          1.0\n",
      "\n",
      "Kendall Tau Correlation Matrix:\n",
      "           Ranking_A  Ranking_B\n",
      "Ranking_A        1.0        0.2\n",
      "Ranking_B        0.2        1.0\n"
     ]
    }
   ],
   "source": [
    "#Example, Pearson Correlation \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate the first feature (100 samples)\n",
    "feature1 = np.random.normal(0, 1, 100)\n",
    "\n",
    "# Generate noise (100 samples)\n",
    "noise = np.random.normal(0, 1, 100)\n",
    "\n",
    "# Create the second feature with a 70% correlation to the first feature\n",
    "# Adjust the weight of the noise to control the correlation\n",
    "feature2 = 0.4 * feature1 + 0.3 * noise\n",
    "\n",
    "# Spearman Rank Correlation \n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Feature1': feature1,\n",
    "    'Feature2': feature2\n",
    "})\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df.corr(method='pearson')\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "print(\"\\nSample DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Simulated data for Spearman correlation\n",
    "data = {\n",
    "    'Exam_Rank': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Study_Hours': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate Spearman correlation\n",
    "spearman_corr = df.corr(method='spearman')\n",
    "print(\"\\nSpearman Correlation Matrix:\")\n",
    "print(spearman_corr)\n",
    "#Exam Rank and Study Hours: A Spearman correlation coefficient close to -1 indicates a strong negative monotonic relationship, \n",
    "#meaning as study hours increase, exam rank decreases (or vice versa).\n",
    "\n",
    "#Kendall Tau Correlation\n",
    "# Simulated data for Kendall Tau correlation\n",
    "data = {\n",
    "    'Ranking_A': [1, 2, 3, 4, 5],\n",
    "    'Ranking_B': [3, 2, 1, 5, 4]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate Kendall Tau correlation\n",
    "kendall_tau_corr = df.corr(method='kendall')\n",
    "print(\"\\nKendall Tau Correlation Matrix:\")\n",
    "print(kendall_tau_corr)\n",
    "#Ranking_A and Ranking_B: A Kendall Tau correlation coefficient close to -1 indicates a strong negative association between the two rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "155bf323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:21.463533Z",
     "iopub.status.busy": "2024-07-22T14:59:21.463127Z",
     "iopub.status.idle": "2024-07-22T14:59:21.480133Z",
     "shell.execute_reply": "2024-07-22T14:59:21.478748Z"
    },
    "papermill": {
     "duration": 0.042236,
     "end_time": "2024-07-22T14:59:21.482650",
     "exception": false,
     "start_time": "2024-07-22T14:59:21.440414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Temperature  IceCreamSales\n",
      "0    32.483571     101.626868\n",
      "1    29.308678     108.821808\n",
      "2    33.238443     126.099480\n",
      "3    37.615149     134.415052\n",
      "4    28.829233     112.091218\n",
      "5    28.829315     123.398278\n",
      "6    37.896064     189.307974\n",
      "7    33.837174     138.840251\n",
      "8    27.652628     115.761520\n",
      "9    32.712800     129.362283\n",
      "Correlation Matrix:\n",
      "               Temperature  IceCreamSales\n",
      "Temperature       1.000000       0.635724\n",
      "IceCreamSales     0.635724       1.000000\n",
      "Covariance Matrix (Positive Covariance):\n",
      "               Temperature  IceCreamSales\n",
      "Temperature      20.619247      70.661570\n",
      "IceCreamSales    70.661570     599.178402\n",
      "\n",
      "Temperature and Ice Cream Sales: A positive covariance indicates that higher temperatures are associated with higher ice cream sales.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Covariance\n",
    "#Example 1: Positive Covariance\n",
    "#Scenario: The relationship between temperature and ice cream sales.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulated data for temperature and ice cream sales\n",
    "temperature = np.random.normal(30, 5, 100)  # Temperature in degrees Celsius\n",
    "ice_cream_sales = temperature * 4 + np.random.normal(0, 20, 100)  # Ice cream sales related to temperature\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    'Temperature': temperature,\n",
    "    'IceCreamSales': ice_cream_sales\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print (df.head(10))\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df.corr(method='pearson')\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Calculate covariance matrix\n",
    "covariance_matrix = df.cov()\n",
    "print(\"Covariance Matrix (Positive Covariance):\")\n",
    "print(covariance_matrix)\n",
    "print (\"\\nTemperature and Ice Cream Sales: A positive covariance indicates that higher temperatures are associated with higher ice cream sales.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edef83a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:21.524222Z",
     "iopub.status.busy": "2024-07-22T14:59:21.523785Z",
     "iopub.status.idle": "2024-07-22T14:59:21.540134Z",
     "shell.execute_reply": "2024-07-22T14:59:21.538972Z"
    },
    "papermill": {
     "duration": 0.040608,
     "end_time": "2024-07-22T14:59:21.542974",
     "exception": false,
     "start_time": "2024-07-22T14:59:21.502366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TV_Hours  PhysicalActivity\n",
      "0  4.764052         61.775230\n",
      "1  3.400157         59.259633\n",
      "2  3.978738         53.860195\n",
      "3  5.240893         52.438052\n",
      "4  4.867558         45.458803\n",
      "5  2.022722         89.490885\n",
      "6  3.950088         58.431021\n",
      "7  2.848643         67.776298\n",
      "8  2.896781         80.646899\n",
      "9  3.410599         73.296589\n",
      "Correlation Matrix:\n",
      "                  TV_Hours  PhysicalActivity\n",
      "TV_Hours          1.000000         -0.879498\n",
      "PhysicalActivity -0.879498          1.000000\n",
      "\n",
      "Covariance Matrix (Negative Covariance):\n",
      "                  TV_Hours  PhysicalActivity\n",
      "TV_Hours          1.026087         -9.672422\n",
      "PhysicalActivity -9.672422        117.873419\n",
      "\n",
      "\n",
      "TV Hours and Physical Activity: A negative covariance indicates that more hours spent watching TV are associated with less physical activity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Coveriance\n",
    "#Example 2: Negative Covariance\n",
    "#Scenario: The relationship between the number of hours spent watching TV and physical activity.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Simulated data for TV hours and physical activity\n",
    "tv_hours = np.random.normal(3, 1, 100)  # TV hours per day\n",
    "physical_activity = 100 - tv_hours * 10 + np.random.normal(0, 5, 100)  # Negative relationship with TV hours\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    'TV_Hours': tv_hours,\n",
    "    'PhysicalActivity': physical_activity\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print (df.head(10))\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df.corr(method='pearson')\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Calculate covariance matrix\n",
    "covariance_matrix = df.cov()\n",
    "print(\"\\nCovariance Matrix (Negative Covariance):\")\n",
    "print(covariance_matrix)\n",
    "print (\"\\n\\nTV Hours and Physical Activity: A negative covariance indicates that more hours spent watching TV are associated with less physical activity.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1669cc2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T14:59:21.584372Z",
     "iopub.status.busy": "2024-07-22T14:59:21.583943Z",
     "iopub.status.idle": "2024-07-22T14:59:21.594725Z",
     "shell.execute_reply": "2024-07-22T14:59:21.593690Z"
    },
    "papermill": {
     "duration": 0.034886,
     "end_time": "2024-07-22T14:59:21.597639",
     "exception": false,
     "start_time": "2024-07-22T14:59:21.562753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Covariance Matrix (Zero Covariance):\n",
      "               BooksRead  MoviesWatched\n",
      "BooksRead       7.960707       0.022121\n",
      "MoviesWatched   0.022121       8.752626\n",
      "\n",
      "\n",
      "Books Read and Movies Watched: A covariance close to zero indicates no linear relationship between the number of books read and the number of movies watched. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Covariance\n",
    "#Example 3: Zero Covariance\n",
    "#Scenario: The relationship between the number of books read and the number of movies watched.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(0)\n",
    "data = {\n",
    "    'BooksRead': np.random.randint(0, 10, 100),  # Number of books read per month\n",
    "    'MoviesWatched': np.random.randint(0, 10, 100)  # Number of movies watched per month\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate covariance matrix\n",
    "covariance_matrix = df.cov()\n",
    "print(\"\\nCovariance Matrix (Zero Covariance):\")\n",
    "print(covariance_matrix)\n",
    "print (\"\\n\\nBooks Read and Movies Watched: A covariance close to zero indicates no linear relationship between the number of books read and the number of movies watched. \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2fab3",
   "metadata": {
    "papermill": {
     "duration": 0.019678,
     "end_time": "2024-07-22T14:59:21.637547",
     "exception": false,
     "start_time": "2024-07-22T14:59:21.617869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5393030,
     "sourceId": 8960144,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.279568,
   "end_time": "2024-07-22T14:59:22.278460",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-22T14:59:11.998892",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
